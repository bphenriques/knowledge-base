<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Scalability #  It can meets increases in demand while remaining responsive.
This is different from performance. Performance optimizes response time (latency) while scalability optimizes ability to handle load. Requests per second actually measures both but we do not know which aspect was improved.
Note Scalability is not the number of requests qwe can handle a in a given period of time (req/sec) but he number of requests itself (load)."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Consistency And Availability"><meta property="og:description" content="Scalability #  It can meets increases in demand while remaining responsive.
This is different from performance. Performance optimizes response time (latency) while scalability optimizes ability to handle load. Requests per second actually measures both but we do not know which aspect was improved.
Note Scalability is not the number of requests qwe can handle a in a given period of time (req/sec) but he number of requests itself (load)."><meta property="og:type" content="article"><meta property="og:url" content="https://bphenriques.github.io/knowledge-base/system-design/consistency_and_availability/"><title>Consistency And Availability | Bruno Henriques</title><link rel=manifest href=/knowledge-base/manifest.json><link rel=icon href=/knowledge-base/favicon.png type=image/x-icon><link rel=stylesheet href=/knowledge-base/book.min.19256db6edae397ffaff8a50dfd331f9f5f916ae4720b444fb87a8d48e9a61f0.css integrity="sha256-GSVttu2uOX/6/4pQ39Mx+fX5Fq5HILRE+4eo1I6aYfA="><script defer src=/knowledge-base/en.search.min.0490e49bbe9ce067b22dd7720d8e50a3a998e3585f89ab97a368bba795094000.js integrity="sha256-BJDkm76c4GeyLddyDY5Qo6mY41hfiauXo2i7p5UJQAA="></script></head><body><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><h2 class=book-brand><a href=/knowledge-base><span>Bruno Henriques</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><span>Concurrency</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/concurrency/actor-model/>Actor Model</a></li></ul></li><li class=book-section-flat><span>Data Processing</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/data-processing/apache-spark/>Apache Spark</a></li><li><a href=https://bphenriques.github.io/knowledge-base/data-processing/states-of-data/>States of Data</a></li></ul></li><li class=book-section-flat><span>Documentation</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/documentation/readme/>readme</a></li></ul></li><li class=book-section-flat><span>JVM</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/jvm/jackson/>Jackson</a></li></ul></li><li class=book-section-flat><span>Learning</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/learning/hands_on_scala_programming/>Hands-on Scala Programming</a></li></ul></li><li class=book-section-flat><span>Messaging Systems</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/messaging-systems/overview/>Messaging Systems Comparison</a></li></ul></li><li class=book-section-flat><span>Monitoring</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/monitoring/bug-management/>Bug Management</a></li></ul></li><li class=book-section-flat><span>Reactive Architecture</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/reactive-architecture/patterns/>Patterns</a></li></ul></li><li class=book-section-flat><span>Security</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/security/jwt/>JWT</a></li></ul></li><li class=book-section-flat><span>System Design</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/system-design/consistency_and_availability/ class=active>Consistency And Availability</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/distribute_systems/>Distributed Systems</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/domain_driven_design/>Domain Driven Design</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/message_driven_architecture/>Message Driven Architecture</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/microservices/>Microservices</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/monolith/>Monolith</a></li><li><a href=https://bphenriques.github.io/knowledge-base/system-design/service_oriented_architecture/>Service Oriented Architecture</a></li></ul></li><li class=book-section-flat><span>Work</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/work/random-memories/>Random Memories</a></li><li><a href=https://bphenriques.github.io/knowledge-base/work/way-of-work/>Way of work</a></li></ul></li><li class=book-section-flat><span>Snippets</span><ul></ul></li><li class=book-section-flat><span>Blog Ideas</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/blog-ideas/using_psql_as_job_queue/>Using PSQL as job queue</a></li></ul></li><li class=book-section-flat><span>Uncategorized</span><ul><li><a href=https://bphenriques.github.io/knowledge-base/uncategorized/akka/>Akka</a></li><li><a href=https://bphenriques.github.io/knowledge-base/uncategorized/lightbend-academy/>Lightbend Academy</a></li><li><a href=https://bphenriques.github.io/knowledge-base/uncategorized/org_protocol/>Org-Protocol</a></li><li><a href=https://bphenriques.github.io/knowledge-base/uncategorized/relevant-xkcds/>Relevant xkcds</a></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/knowledge-base/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Consistency And Availability</strong>
<label for=toc-control><img src=/knowledge-base/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#scalability>Scalability</a></li><li><a href=#consistency>Consistency:</a><ul><li><a href=#eventual-consistency>Eventual Consistency</a></li><li><a href=#strong-consistency>Strong Consistency</a></li></ul></li><li><a href=#cap-theorem>CAP Theorem</a><ul><li><a href=#partition-tolerance>Partition Tolerance</a></li></ul></li><li><a href=#sharding-as-a-way-to-have-strong-consistency>Sharding as a way to have strong consistency</a><ul><li><a href=#effects>Effects</a></li><li><a href=#failure>Failure</a></li></ul></li><li><a href=#crdts-provide-a-availability-solution-based-on-async-replication>CRDTs provide a availability solution based on async replication</a></li></ul></nav></aside></header><article class=markdown><h2 id=scalability>Scalability
<a class=anchor href=#scalability>#</a></h2><p>It can meets increases in demand while remaining responsive.</p><p>This is different from performance. Performance optimizes response time (latency) while scalability optimizes ability to handle load. Requests per second actually measures both but we do not know which aspect was improved.</p><p><strong>Note</strong> Scalability is not the number of requests qwe can handle a in a given period of time (req/sec) but he number of requests itself (load).</p><p>If x axis is number of requests (Load) and y axis is response time. Improving performance leads to decrease in the y axis. Improving scalability means a shift on the x axis meaning that we can handle more requests with the same response time.</p><p>Still confused as if I improve performance I should free up resources to handle more requests..</p><p>Reactive Microservices focus on improving scalability.</p><h2 id=consistency>Consistency:
<a class=anchor href=#consistency>#</a></h2><p>All members of the system have the same view or state. This does not factors time.</p><h3 id=eventual-consistency>Eventual Consistency
<a class=anchor href=#eventual-consistency>#</a></h3><p>Guarantees that, in the absence of new updates, all accesses to a specific piece of data will eventually return the most recent data.</p><p>Different forms:</p><ul><li>Eventual Consistency</li><li>Causal Consistency</li><li>Sequential Conssitency</li><li>others</li></ul><p>E.g., Source control are eventually consistent. All the code reading is potentially out-of-date and a merge operations is relied upon to bring the local state back to speed.</p><h4 id=todo-check-each-one>TODO: Check each one
<a class=anchor href=#todo-check-each-one>#</a></h4><h3 id=strong-consistency>Strong Consistency
<a class=anchor href=#strong-consistency>#</a></h3><p>An update to a piece of data needs agreement from all nodes before it becomes visible.</p><p>Physically it is impossible therefore we simulate: Locks that introduce overhead in the form of contention. Now it becomes a single resource which eliminates the distributed nature. Tying the distributed problem to a non-distributed resource.</p><p>Traditionally, monoliths are based around strong consistency.</p><h4 id=effects-of-contention>Effects of contention
<a class=anchor href=#effects-of-contention>#</a></h4><p>Definition: Any two things that contend for a single limited resource and only one will win and the other will be forced to wait.</p><p>As the number of resources disputing for the resource, more time time it will take to finally free up the resources.</p><ul><li><p>Amdahl&rsquo;s law</p><p>In short, contention limits paralelization.</p><p>Defines the maximum improvement gaines by parallel procesing. Improvements from paralelization are limited to the code that can be paralelized. Contention limits such paralism reducing the advantages of the improvements. Does not matter as long as the contention exist.</p></li></ul><ul><li><p>Coherence Delay</p><p>Definition: Time it takes for synchronization to complete on a distributed systems - My definition following below notes:</p><p>Syncronization is done using crosstalk or gossip - Each system sends messages to each other node informing of any state changes. The time it takes for the cynscronization to complete is called <strong>Coeherency Delay</strong>.</p><p>Increasing the number of nodes increases the delay.</p></li></ul><ul><li><p>Gunther&rsquo;s Universal Scalability Law</p><p>Increasing concurrency can cause negative resutrns due to contention and coherency delay.</p><p>Picks from Amdahl&rsquo;s law. In addition to contention, it accounts for coeherency delay.</p><p>As the system scales up, the cost to coordinate between nodes exceeds any benefits.</p></li></ul><ul><li><p>Laws of scalability</p><p>Both these laws demonstrate that linear scalability is almost always unachivable. Such is only possible if the system lieve in total isolation.</p></li></ul><ul><li><p>Reactive Systems</p><p>Reduce contention by:</p><ul><li>Isolating locks</li><li>Eliminating transactions</li><li>Avoiding blocking operations</li></ul><p>Mitigate coherency delays by:</p><ul><li>Embracing Eventual Consistency</li><li>Building in Autonmy</li></ul><p>This allows for higher scalability as we reduce or eliminate these factors.</p></li></ul><h2 id=cap-theorem>CAP Theorem
<a class=anchor href=#cap-theorem>#</a></h2><p>States that a distributed system cannnot provide more than than two of the following:</p><ul><li>Consistency</li><li>Availability</li><li>Partition Tolerance</li></ul><p>One has to pick one of the following combinations:</p><ul><li>(CP) Consistent and Partition Tolerance</li><li>(AP) Available and Partition Tolerance.</li></ul><p>In practice, they may claim CP/AP except for some edge-cases. It is a balance.</p><h3 id=partition-tolerance>Partition Tolerance
<a class=anchor href=#partition-tolerance>#</a></h3><p>The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network.</p><p>They can occur due to:</p><ul><li>Problems in the network.</li><li>When a node goes down.</li></ul><p>May be short or long lived.</p><p>Two options:</p><ul><li>(AP) Sacrifice Consistency: Allow writes to both sides of the partition. This require merging the data in order to restore consistency.</li><li>(CP) Sacrifice Availability: Disabling or terminating on side of the partitions. During this, some or all of your system will be unavailable.</li></ul><h2 id=sharding-as-a-way-to-have-strong-consistency>Sharding as a way to have strong consistency
<a class=anchor href=#sharding-as-a-way-to-have-strong-consistency>#</a></h2><p>Limit the scope of the contention and reduce crosstalk. Is applied within the application. It is not the same type of sharding used in some databases, the technique is similar though.</p><p>Allows strong consistency.</p><p>Partitions entities (or Actors) in the domain according to their id.</p><p>Groups of entities are called a shard and each entity only exists in one shard.</p><p>Each shard exists in only one location. This fact eliminates the distributed systems problem.</p><p>The entity acts as a consistency boundary.</p><p>In order for this to work, we need to have a coordinator that ensures that traffic for a particular entity is routed to the correct location. The coordinator uses the ID to calculate the appropriate shard.</p><p>Aggregate Roots are good candidate for sharding.</p><p>It is important to have a balanced shards and that requires a good sharding key - UUIDs or hashcodes. Poor key selections will result in hotspots.</p><p>Rule of thumb: 10x as many shards as nodes.</p><p>Akka provides this as a means to distribute actors across a cCLuster in a shared setup. Lagom persistent entities levarage akka cluster sharding to distribute the entities across the cluster.</p><p>What about resharding? when a system goes down&mldr;</p><p>Sharding allows a great caching solution as:</p><ul><li>We can store the cache results after writing to the database</li><li>Databases is effectively write-only which can speed up things</li><li>We only consult the cache during reads.</li><li>Begs the question: How many items and what is the TTL? Well.. it for certain reduces the read on the DB but that is not forever unless we have infinite memory.</li></ul><h3 id=effects>Effects
<a class=anchor href=#effects>#</a></h3><ul><li>Does not eliminate contention. It solely isolates to a single entity.</li><li>The router/coordinator represents a source of contention as well.</li><li>A shareded system minimizes contention by:<ul><li>Limiting the amounf of work the router/coordinator performs - By storing where the shard is after asking the coordinator - How to invalidate that cache due to failures?</li><li>Isolates contention to individual entities</li></ul></li></ul><p>Scalability is doen by distributing the shards over mode machines.
Strong consistency is achiaved by isolating operations to a specific entity.
Careful choice of shard keys is important to maintain a good scalability.</p><h3 id=failure>Failure
<a class=anchor href=#failure>#</a></h3><p>Sharding sacrifices availability. Once a shard goes down, there will be a period of time where it is unavailable and wil migrate to another node eventually.</p><h2 id=crdts-provide-a-availability-solution-based-on-async-replication>CRDTs provide a availability solution based on async replication
<a class=anchor href=#crdts-provide-a-availability-solution-based-on-async-replication>#</a></h2><p>Conflict-free Replicated Data</p><p>On the application level.</p><p>Highly available and eventually consistent.</p><p>Specially designed data type.</p><p>Updates are applied on one replica and then copied async.</p><p>Udpdates are merged to determine the final state.</p><p>Two types:</p><ul><li>CvRDT - Convergent Replicated Data Type copy state between replicas. Requires a merge operation that understands how to combine two states. These operations must be: commutative, associative and idempotent.</li><li>CmRDT - Commutative Replicated Data Types. These copy operations isntead of state.</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><div><a class="flex align-center" href=https://github.com/bphenriques/knowledge-base/edit/master/org/system-design/consistency_and_availability.org target=_blank rel=noopener><img src=/knowledge-base/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#scalability>Scalability</a></li><li><a href=#consistency>Consistency:</a><ul><li><a href=#eventual-consistency>Eventual Consistency</a></li><li><a href=#strong-consistency>Strong Consistency</a></li></ul></li><li><a href=#cap-theorem>CAP Theorem</a><ul><li><a href=#partition-tolerance>Partition Tolerance</a></li></ul></li><li><a href=#sharding-as-a-way-to-have-strong-consistency>Sharding as a way to have strong consistency</a><ul><li><a href=#effects>Effects</a></li><li><a href=#failure>Failure</a></li></ul></li><li><a href=#crdts-provide-a-availability-solution-based-on-async-replication>CRDTs provide a availability solution based on async replication</a></li></ul></nav></aside></main></body></html>