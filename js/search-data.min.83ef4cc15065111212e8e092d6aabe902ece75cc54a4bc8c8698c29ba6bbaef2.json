[{"id":0,"href":"/knowledge-base/notes/actor-model/","title":"Actor model","tags":["concurrency"],"content":"Concept from 1973 by Carl Hewitt:\n Actor is the fundamental unit of computation embodying processing, storage, and communication.\n Fundamentals #   All computation occurs inside of the actor. Each actor has an address. Actors may create new actors, send messages to them and changing their own behavior to handle new messages (e.g., change the state).  An actor does not exist isolated, it works in tandem with others and are arranged in hierarquy:\n Actors can split up and delegate tasks to child actors. Child actors are supervised and delegate their failures back to their parent.  Anatomy #  Each actor has:\n An address. An mailbox. An dispatcher.  Messages are sent to an Actor Address which are enqued in the mailbox and only then are dispatched for processing. Note that, only one message is dispatched at a time leading to the ilusion of single thread (there may be multiple actor within the same system, each one handling messages in different threads).\nIt is important that that actors communicate exclusively through messages and do not shared state to ensure Strong Consistency! Moreover messages must be immutable.\n Reactive Systems #  In the context of Reactive Systems, Actor model is a reactive tool, a paradigm that:\n Actors only communicate through asyncronous messages. Message driven - All communication between actors is done with async non-blocking messages. Abstractions provide elasticity and resiliency.  Akka uses the actor model - https://doc.akka.io/docs/akka/current/typed/guide/actors-intro.html?language=scala\nThe message driven system provides location transparency, i.e., the technique remainins the same regardless of where the actors are. This allows better resiliency and elastic (hmm.. questions on this bit). This is different from \u0026ldquo;Transparent remoting\u0026rdquo; as this hides potential networking issues while making it seem like local calls. Location transparency makes the opposite which is makes local calls seem like remote calls, therefore whoever uses is aware that there are potential failures.\nAkka is an implementation of the Actor Model.\n","preview":"Concept from 1973 by Carl Hewitt:\n Actor is the fundamental unit of computation embodying …"},{"id":1,"href":"/knowledge-base/notes/akka/","title":"Akka","tags":["akka"],"content":"Akka #  Toolkit and runtime for building highly concurrent, distributed and fault tolerant message-driven application in the JVM. It can be used to build Reactive Systems. Proposes unified programming model for:\n Simpler concurrency: single threaded illusion as each actor processes a message at a time (no need to locks or synchronization strategies). Simpler distribution: is distributed by default (see more on Akka Cluster ). Simpler fault tolerance: Decouples communication from failure handling.  ","preview":"Akka #  Toolkit and runtime for building highly concurrent, distributed and fault tolerant …"},{"id":2,"href":"/knowledge-base/notes/akka_actors/","title":"Akka actors","tags":["akka"],"content":"Handling Messages Asyncronously #  Blocking threads inside the actors creates contention therefore the handling of the messages must happen in a async fashion, including any DB writes and DB reads when starting the actor. For example, using interfaces such as Future[T]. However this may lead to concurrency within the actor itself removing the ilusion of a single thread. This means that messages must be stash until other operations complete.\nIn essence the actor has 3 states:\n Loading - Load the state from the DB. Running - Regular behavior. Waiting.  Question: If DB fails, then it is recommended to throw the exception leading to a restart of the Actor that in turn will re-read the state from the DB. So:\n It is explained that the stash is not lost, how? What happens if there is a persistent issue in the DB? Will there be a loop?  Akka Actor Lifecycle #  Can be stopped by himself or by others.\nLifecycle (without faults):\nhook:preStart() -\u0026gt; started -[stop]-\u0026gt; stopped -\u0026gt; hook:postStop() -\u0026gt; terminated The actor is created assyncronously and available right away through the ActorRef.\nStopping an actor will:\n Finishes the processing the current message. Suspends message processing. Stop its children - See Actor Model . Waits for their termination confirmations and then stops himself.  How to stop: PoisonPill (context.stop(self())) and actorRef ! Kill messages (throw ActorKilledException). They are not appropriate to perform a cleanup before shutting down as the Actor does not \u0026ldquo;see\u0026rdquo; those messages (it is handled internally). It is best to use a dedicated message such as StopMeGraciouslyMessage.\nMonitor #  Dead Watch allows monitoring another actor\u0026rsquo;s termination (regular Terminated message in the receive block).\nFailure Handling #  Akka deals with failures at the level of the individual actor (bulkheading has it only affects that actor).\nDoes not throw the message back to the sender (b/c the sender does not know how to handle it). Instead the error is sent to a responsibile entity (e.g., \u0026ldquo;Manager\u0026rdquo;) that determines the required steps to recover.\nWhen an actor fails, Akka provides two configurable strategies:\n OneForOneStrategy: Only the faulty child is affected. AllForOneStrategy: All children are affected by one faulty child.  Both are configured with a type Decider = PartialFunction[Throwable, Directive]. If not defined a directive, then the parent is consired faulty. Where Directive:\n Resume: Resume message processing. Use if the state remains valid. Restart: Start a new actor in its place and resume, however all childs are stopped (by default unless preRestart hook is changed). It supports max number of retries and within a time limit. Stop. Escalate: Delegate the decision to the supervisor\u0026rsquo;s parent.  By default it is OneForOneStrategy with some directives that are too specific to group here and we can check the documentation. In short, by default, the actor will be restarted. In any case, message processing is suspended.\n All descendants of the actor are suspended. The actor\u0026rsquo;s parent handles the failure.  Proper tuning leads to a self-healing system. Some exceptions are worth stopping the actor while others are worth recovering.\nFull Lifecycle #   Ask vs Tell #  Ask: actorRef ? Message Tell: actorRef ! Message\nUse Ask when:\n Bridging non-actor code to actor-code (e.g., bridging with HTTP controllers ?). We are expecting a response within a timeout. In this case we use actorRef ? Message pipeTo self which in turn will will handle the response, e.g., val receive: Receive = { case MessageResponse =\u0026gt; stuff }.  Use tell when:\n We do not care about the response.  Neverthless, when using the ask operator, always ue pipeTo within the actor system to avoid breaking the single thread illusion.\nTesting #  See Akka Testing .\n","preview":"Handling Messages Asyncronously #  Blocking threads inside the actors creates contention …"},{"id":3,"href":"/knowledge-base/notes/akka_cluster/","title":"Akka cluster","tags":["akka"],"content":"Akka Cluster #  Scenario: Make actors communicate across the network.\nAllows actors to communicate across the network, greatly simplifying the process. Each node represents an actor system and they all share the same name.\nAkka Cluster Aware Routers #  Scenario: High workload.\nScalling vertically has limits. Introducing Akka Cluster Aware Routers, that allows scalling the system horizontally. I.e., large tasks are broken on smaller tasks that are routed to an especific node of our cluster.\nAkka Cluster Sharding #  Scenario: Database becomes the bottleneck.\nMany applications leverage the database for strong-consistency. However it may become the source of contention (see Amdah\u0026rsquo;s Law ) as the load increases. We may migrate to a cache service (e.g., Memcached or Redis) but it will eventually become the bottleneck.\nIn order to solve this, Akka Cluster Sharding allows distributing Actors across the cluster responsible for managing the state of a especific database entitiy (given a hashing function). With the aid of a single-thread illusion, we can cache the entities in-memory without the risk of desyncronizing with the database, leading to strong consistency. This is great as most applications are read-heavy as opposed to write-heavy.\nAkka Distributed Data #  Scenario: Critical information that is required continously and we need maintain it. Especially small data sets with infrequent updates that require high availabiltiy.\nAkka Distributed Data is a local, replicated and in-memory data storage. The data is asyncronously replicated to other nodes. The consistency model varies and is configurable . Through this, we can perform updates from any node without coordination and any concurrent updates will be automatically resolved by a monotic merge function explicitly provided.\nFor this end we use a especific data-structure called Conflict Free Replicated Data Types (CRDTs).\nFor more, please check here .\nConflict Free Replicated Data Types (CRDTs) #  CRDTs are stored in-memory and can be copied to disk to speed up recovery if a replica fails.\n A marker that shows something was deleted. Can result in data types that only get larger and never smaller. Aka CRDT Garbage  Limitations CRDT: Do not work with every data type that require a merge function. Some data types are too complex to merge and require the use of tombstone.\nLimitations #   It may not be possible depending on the data model due to the merge function. Eventual Consistency. Strong consistency is possible at the expense of availability. The number of top-level entries should me limited (\u0026lt; 1 million) given that it must be transferred to to the nodes. The entity musn\u0026rsquo;t be large given that its full-state may replicated to other nodes.  For more, please check here .\nTODO Concrete use-cases for Akka Distributed Data #  The lack of Akka Distributed Data may lead to frequent network requests to fetch as especific entity. However, its usage also requires querying several nodes to look for a quorum. Both options have drawbacks, I am curious on knowing the decision thought behind it.\nAkka Address #  May be local or remote in the form: akka://\u0026lt;ActorSystem\u0026gt;@\u0026lt;HostName\u0026gt;:\u0026lt;Post\u0026gt;/\u0026lt;ActorPath\u0026gt;\nSeveral protocols are available and depend on the use-case:\n aeron-udp: High throughput and low latency. tcp: Good thorughout and latency but lower. tls-tcp: When encryption is required.  Joining a Cluster #  Requires \u0026ldquo;Seed Nodes\u0026rdquo;, i.e., contact nodes. Any node is eligible. Best practice is to use \u0026ldquo;Akka Cluster Bootstrap\u0026rdquo; to avoid setting static seed-nodes in each configuration file.\nMust be enabled! And it does not bring any advantage until we set the application to leverage this:\nval loyaltyActorSupervisor = ClusterSharding(system).start( \u0026#34;shared-region-name\u0026#34;, MyActorActor.props(someProp), ClusterShardingSettings(system), MyActorSupervisor.idExtractor, MyActorSupervisor.shardIdExtractor ) Akka Cluster Management #  Set of tools served through a HTTP Api to manage the cluster. Must start after the actor system.\nMust be enabled!\nAkka Discovery #  Service to locate and discover services.\nAkka Cluster Bootstrap #  Automated seed node discovery using Akka Discovery.\nHealth Check Endpoints #  Useful when integrating with orchestrating platforms (e.g., K8S).\nCommunication #  It is done by using Gossip Protocol .\nNetwork Partitions #  This issue cannot be recovered by simply rebooting the affected node. In order to fix this:\n Decide which partitions needs to be cleaned up - How? Shutdown the members Inform the cluster that those members are down - PUT -F operation=down /cluster/members/\u0026lt;member address\u0026gt;. Create new members to replace the old.  Step 2. is important otherwise it continues to operate unware that it has been removed from the cluster which can lead to multiple copies of the same shard.\nSplit Brain #  Occurs when single cluster splits into two or more distinctive clusters. It normally does not occur unless poor management (not stopping processes that are Down) or configuration (there are strategies to solve this automatically). Can be caused by improper Downing a member leading to the node creating another cluster as the process was not terminated.\nIt may also occur with a network partition. If this extend, the Unreachable Nodes will be marked as downed but will not be terminated.\nSimpler solutions may be solved automatically through orchestration platforms that automatically stop the process. More complicated split brains may be solved using Lightbend Split Brain Resolver.\nWhen using sharding or singleton for data consistency #  Each cluster can have a copy of the actor leading to a inconsistency and data corruption specially if both shards have access to the database.\nLighbend Split Brain Resolver #  Set of customizable strategies for terminating members in order to avoid Split Brain scenarios. Terminating members allow orchestration platforms to take over and heal the problem.\nStatic Quorum #  Fixed sized quorom of node. All nodes will evaluate their situation and Down unreachable. If quorum is set then a smaller cluster will prevail, otherwise the nodes will shutdown themselves. The quorum value must at least n/2 + 1.\nKeep Majority #  Similar to previous but dynamically tracks the size of the cluster.\nKeep Oldest #  Monitors the oldest node in the cluster. Members that are not communicating with that node will be marked as down and the nodes will terminate themselves. If the oldest node has crashed so will the cluster but is configurable in a way, that in that case only the oldest will be Downed.\nKeep Referee #  Similar to the other one but designate a specific node as referee (based on its address). As far as I can see, it is not configurable to avoid crashing the cluster if the referee is down.\nDown Allows #  All nodes terminate themselves relying on good orchestration tools to reduce downtime - Me not like this one.\nLease Majority #  Reserved for Kubernetes deployments.\nIt uses a distributed lock (lock) to make it\u0026rsquo;s decision. Each partition will attempt to obtain it the loser terminates and the winnner remains.\nThere is a bit of nice hack (IMO but can\u0026rsquo;t understand exactly how this is achieved) which is that the side that is theoretically smaller will delay the attempt to obtain the lock so that the majority wins.\nSome Edge Cases #   Indirect connected Edges (for some reason is connected to only one member). Unstable nodes (keeps on disconnecting from some nodes).  These edge-caes are automatically handled.\nOrphaned Node #  Is down but not terminated.\nTODO Cluster Singleton #  ","preview":"Akka Cluster #  Scenario: Make actors communicate across the network.\nAllows actors to …"},{"id":4,"href":"/knowledge-base/notes/akka_dispatcher/","title":"Akka dispatcher","tags":["akka","dispatcher"],"content":"The engine of Akka as it decides when a actor should process messsages and when it must yield the thread for others. It means that dispatchers are in control of the thread time and of the threads themselves.\n Dispatcher (default): Event-driven dispatcher, sharing threads from thread pool. PinnedDispatcher: Dedicated thread per actor. CallingThreadDispatcher: Just for testing.  The best tips are:\n Adjust throughput to deliver more messages to the actors before yielding. Adjust dispatcher settings according to the use-case.  ","preview":"The engine of Akka as it decides when a actor should process messsages and when it must …"},{"id":5,"href":"/knowledge-base/notes/akka_router/","title":"Akka router","tags":["akka","router"],"content":"Intro #  In Akka, Router routes messages to destination Akka Actors called routees that can process messages in parallel to improve throughput. The way routing is done may be configured to fit the use-case.\nRouting Strategies #  Determines how router routes to its routees.\n RandomRoutingLogic: Pure random and fast. May be unbalanced. RoundRobinRoutingLogic: Take turns. Is more fair and distributed. But depending on the messages, some actors may have more work than others. SmallestMailboxRoutingLogic: Tracks the size of the mailbox. ConsistentHashingRoutingLogic: Messages go to a specific routee. BroadcastRoutingLogic: Sends to all routees. ScatterGatherFirstCompletedRoutingLogic: Similar to broadcast but looks for the first response. This optimizes speed as the picks the first one. TailChoppingRoutingLogic: Similar to previous but delays a bit before sending to the next routee.  Types of Router #  How the routees are managed. In this case, the message delivery is optimized:\n Messages are not enqueued into the router\u0026rsquo;s mailbox but are delivered directly to the routees. but delive  Pool Router #  Creates and supervises a number of routees according to a configuration. E.g., create 5 routees. It can be configured to dynamically adjust the number of routees.\nGroup Router #  Router is configured to route to existing actors (routees). Each actor is supervised by the parents and not by the router as opposed to previous type. In this case we need to especify the routees and will find all that match.\nSpecial Messages #   PoisonPill: Is never send to the routee. In this case, this only kills the router. The outcome of the routees will depend on the type of router. On Pool Router (routees are supervised by the router), they are also go with it. On Group Router the router will kill but the routees will not because are not supervised by it. Kill: Ditto. Broadcast: Delivers to all routees regardless of the routing logic. We can use it to stop all routees by wrapping a PoisonPill inside a Broadcast message.  ","preview":"Intro #  In Akka, Router routes messages to destination Akka Actors called routees that …"},{"id":6,"href":"/knowledge-base/notes/akka_sharding/","title":"Akka sharding","tags":["akka"],"content":"Akka Cluster Sharding #  Distribute actors across a cluster:\n Entities: The main unit (e.g., UserId) Shards: Holds entities (e.g., each shard holds 10 UserIds). Shard Region: Holds Shards. Shard Coordinator: Manages shards.  Entity #  The main unit is: Entity identified by the EntityId which in essence represents the aggregate root\u0026rsquo;s identifier of a concept of our domain (e.g., UserId) and is unique within the cluster. This leads to Strong Consistency given that Akka provides the single thread illusion.\nThe EntityId is extracted through the ExtractEntityId partial function. This is often modeled using a Envelope (which is not mandatory when the message contains the identifier):\ncase class Envelope(entityId: String, message: Any) val idExtractor: ExtractEntityId = { case Envelope(id, msg) =\u0026gt; (id, msg) } The name of the actor essentially becomes the EntityId.\nShards #  In turn, these entities are grouped into shards. The distribution depends on the its identifiers (ShardId) are generated which is usually based on EntityId. An improper distribution leads to a unbalanced cluster, which leads to hotspots (e.g., names, dates, auto-incrementing ids). As rule of thumb, use ~10 shards per node as too many may be costly to find them and too short reduces the capability to distribute them.\nIn general, the identifier shard identifier is modelled as follows:\nval shardIdExtractor: ExtractShardId = { case Envelope(id, _) =\u0026gt; (Math.abs(id.hashCode % totalShards)).toString } Shard Region #  For a type of entity, there is usually one Shard Region per JVM, in other words, we instantiate a shard region (for a given actor type) on every node that we want to host shards. Alternatively, we may merely create a proxy to a shard region. Last but not the least, a shard region is a ActorRef that we can send messages as usual.\nShard Coordinator #  Manages shards. It is responsible to route the messages addressed to a specific entity. It provides the location of the shard which can then be used to acccess the entity.\nRuns as an Akka Cluster singleton and it does not do much of work therefore rarely becomes a bottleneck.\nFrom Stateless systems to Stateful systems #  Strong consistency in a stateless system is done by leveraging a DB as the source of truth, to maintain Strong Consistency. However, as the system grows the DB may become a bottleneck. We can leverage Sharding:\n Load is distributed across multiple machines in the cluster. State can be cached for fast access with no need to read from DB. Strong Consistency is guaranteed by sharded actors backed by the single thread illusion.  The actors may now become a source of contention, however it is distributed across multiple machines. This means that the system is Elastic and we can scale as needed.\nWhen there is a failure, it is located in a single actor - Bulkheading.\nStateful Actors #  Read may be done directly from the state. But writes may have to go through the DB followed by a update of our internal state. This is important as when the Actor fails, we need to rebuild our internal state from DB.\nIn order to avoid blocking the system while reading from the DB we can use the Stash to store incoming messages until we rebuild the internal state. See following example:\nclass MyActor(repository: Repository) extends Actor with Stash { var state: Option[State] = None repository.read(id) // Non-blocking DB. Asyncronous read from DB.  .map(state =\u0026gt; StateLoaded(state)) // Notify so that we can change the current context  .pipeTo(self) def receive: Receive = loading def loading: Receive = { case StateLoaded(s) =\u0026gt; state = Some(s) unstashAll // read all accumulated messages as soon as we process this message \tcontext.become(running) case State.failure(ex) =\u0026gt; throw ex // Best practice: trigger the restart of the actor by default. The stashed messages are not lost.  case _ =\u0026gt; stash() } def running: Receive = { //regular handler  } } Now when we introduce operations that need to update the DB we need to fine-tune this.\ncase class UpdateState(foo: Int) case class StateUpdated(state: Option[State]) class MyActor(repository: Repository) extends Actor with Stash { var state: Option[State] = None repository.read(id) // Non-blocking DB. Asyncronous read from DB.  .map(state =\u0026gt; StateLoaded(state)) // Notify so that we can change the current context  .pipeTo(self) def receive: Receive = loading def loading: Receive = { case StateLoaded(s) =\u0026gt; state = Some(s) unstashAll // read all accumulated messages as soon as we process this message \tcontext.become(running) case State.failure(ex) =\u0026gt; throw ex // Best practice: trigger the restart of the actor by default. The stashed messages are not lost.  case _ =\u0026gt; stash() } def running: Receive = { case UpdateState(foo) =\u0026gt; context.become(waiting) repository.update(state.copy(foo = foo)) .map(StateUpdate.apply) .pipeTo(self)(sender()) // send message to self as soon as the operation is done with the original sender() to reply back to  } def waiting: Receive = { case evt @ StateUpdate(state) =\u0026gt; unstashAll() // enqueue messages not processed while the DB was being written. \tcontext.become(running) // can process messages as usual \tsender() ! evt // reply back to the original sender  case failure @ Status.Failure(ex) =\u0026gt; log.error(s\u0026#34;[orderId] FAILURE: ${ex.getMessage}\u0026#34;) sender() ! failure // make sure we reply back \tthrow ex // trigger actor restart  case _ =\u0026gt; stash() } Passivation #  This fenomenon can be observer through small dips in the throughput. This happens as the Actors attempts to manage the number of actors in-memory as keep all of them is unreasonable. E.g., idle actors.\nEach actor tracks the time it processed a message. If it hadn\u0026rsquo;t processed a message within a configured time period, it will Passivate, leading to the removal of the actor in-memory.\nThe period must be tune-up, too long may lead to OOM and too short may lead to constant reads from the DB. Best practise is to determine and then tune up by watching the memory usage.\nIt can also be done manually by sending a Passivate message to the parent.\nRebalancing #  Occurs whenever the size of the cluster changes. The Shard coordinator will initiate the rebalancing process by distributing the shards across the all available nodes in order to keep an even distribution of entities.\nThis can only occur in a healthy cluster. Therefore any unreachable nodes must be removed (and terminated before) either manually through Akka Management or using the Lightbend Split Brain Resolver.\nSteps:\n Coordinator informs Regions that a rebalance has started. Messages to an entity on a moving shard are buffered. Oce shared was rebalanced, the queued messages are sent  During rebalancing, the messages delivered follow the at-most-once semantics.\nThere are several Shard Allocation Strategies, the default one is LeastShardAllocationStrategy.\nHowever, the shards are not automatically restarted. In order for it to happen one needs to use \u0026ldquo;Remember Entities\u0026rdquo; with some costs.\nRemember Entities #  By enabling remember-entities, when a node restarts/rebalances, it will restore entities. This works by informing every member every time each entity starts or stops (using Akka distributed data) and stored in a durable storage in the disk (it can be recovered even after full cluster restart). However, this be disabled on environments without persistent storage (e.g., Kubernetes), in those cases use eventsourced data mode (see documentation ).\nWarning!\n Enabling this disables automatic passivation. It is not cheap as every node will have to be informed of all running entities, which leads to an overhead starting/stopping them.  Best practice is to limit when we have a limited number of active entities. Most of times is not really needed as entities will be removed automatically through Passivation brought back when needed. However some use-cases:\n When the entity has a scheduled process that may not have completed. When the time to restart an entity on demand could cause the system to backup (long startup times). When the resource savings of passivating the Entities are insignificant.  With this feature, the node\u0026rsquo;s ExtractShardId function must handle ShardRegion.StartEntity(entityId).\nNote: During startup, some nodes may become overwhelmed. In order to avoid concentrating the shards on a single member of the cluster we set the minimum number of members under cluster settings. This allows unnecessary rebalances during the startup. While the cluster has not enough members, existing members will remain in the Join state.\n","preview":"Akka Cluster Sharding #  Distribute actors across a cluster:\n Entities: The main unit …"},{"id":7,"href":"/knowledge-base/notes/akka_streams/","title":"Akka streams","tags":["akka","stream"],"content":"Actor Streams leverage the actor system to consume streams of data. In fact, each element of a stream is a Message within the Actor System\nHere, data flows through a chain of processing stages:\n Sources: The \u0026ldquo;source\u0026rdquo; (e.g., CSV file). Sinks: The \u0026ldquo;destination\u0026rdquo; (e.g., a file). Flows: Transformations made to the data within the Stream (e.g., total number of lines). Runnable Graphs: A stream where all inputs and outputs are connected.  All these stages are composable and, in order to start the flow they have to be materialized. As soon they are connected, each staga materializes a value. See available operators here .\nMoreover, there are two types of Streams:\n Linear Streams: Linear flow. Graphs: Where there may be branches points in the Stream (aka Junctions). Useful for more complex use-cases.  In any case, by default the stages in a Linear Stream run syncronously inside a single together (\u0026ldquo;Fused\u0026rdquo; together) but can also be configured to run asyncronously in separate actors.\nLast but not he least: backpressure is managed by a pull/push mechanism. I.e.:\n Subscriber signals demand which is sent upstream via subscription. Publishers receives demand and pushes data (if available) downstream.  Source #  Stage with single output: Source[+Out, +Mat]:\n Out: The type of each element that is produced. Mat: Type of the materialized value. Usually NotUsed.  Source only push data as long as there is demand. The source will have to deal with incoming data until demand resumes (how how largely demands on the use-case).\nSee available operators here .\nSink #  Stage with a single input: Sink[-In, +Mat]:\n In: The type of each element that is consumed. Mat The type of each element that is produced. E.g., Future[Int].  It creates backpressure by controlling Demand. Note that if the stream is infinite, these sinks may never complete.\nSee available operators here .\nFlows #  Single input and single output: Flow[-In, +Out, +Mat].\nActs both as producer and consumer therefore it propagates demand to the producer as well propagating (and transforming) messages produced to downstream stages.\nThe most notable operators (way too many):\n Simple Operators  Timer Driven  Asyncronous  Backpressure Aware  Nesting and flatenning  Time Aware  Fan-in  Fan-Out   Note that some of these operations are directly accessible from Source and does not require additional typing.\nAdditional notes:\n Buffer smooths flow inconsistencies. extrapolate to deal with slow producers. batch to deal with slow consumers. conflate which creates a summary of the elements when the producer is faster - What is the usefulness?  Runnable Graphs #  Connects source, flows and sinks so that data can start flowing.\nThis is done using via followed to and finally run on a Source as follows:\nimport $ivy.`com.typesafe.akka::akka-actor:2.6.3` import $ivy.`com.typesafe.akka::akka-stream:2.6.3` import akka.actor.ActorSystem import akka.stream.scaladsl.{Flow, Sink, Source} implicit val system = ActorSystem(\u0026#34;QuickStart\u0026#34;) Source(1 to 10) .via(Flow[Int].map(_ * 2)) .to(Sink.foreach(println)) .run Notes:\n via: connects Flow to a Source returning a new Source. Also allows composing two Flows. to: connects a Sink to a Source returning a RunnableGraph. Also connects to a Flow to build new Sink. In essence, it materializes the value from the stage is called on. The run is a terminal operator. There are others.  While the flow is running, values are materialized. These values are then acessed using to, source.toMat(Sink)(Transform/CombineFunction), ~source.viaMat(flow)(Transform/CombineFunction).\nFinally, there are some shortcuts:\n Source.runWith(Sink). Source.runForeach(Function). Source.runFold(0)(_ + _). Source.runReduce(_ + _).  Fault Tolerancy - TODO: Review Examples #  Default strategy is to stop processing the stream and can be overriden within the ActorMaterializer by passing a decider that given an exception it either decides:\n Stop: terminate with an error. Resume: Drop the failing element. Restart: The element is dropped and the stream continues after restarting the stage. Any state acumulated by that stage will be cleared.  Via attributes, each stage can be fine-tuned:\n Dispatcher Buffer Sizes Log Level Supervision  With a Supervision Strategy:\nimport $ivy.`com.typesafe.akka::akka-actor:2.6.3` import $ivy.`com.typesafe.akka::akka-stream:2.6.3` import akka.NotUsed import akka.actor.ActorSystem import akka.stream.{ActorAttributes, Supervision} import akka.stream.scaladsl.{Flow, Sink, Source} import java.lang.ArithmeticException val decider: Supervision.Decider = { case _: ArithmeticException =\u0026gt; Supervision.Resume case _ =\u0026gt; Supervision.Stop } val possibleDivisionByZero = Flow[Int].map(i =\u0026gt; 100 / i) .withAttributes( ActorAttributes.supervisionStrategy(decider) ) implicit val system = ActorSystem(\u0026#34;QuickStart\u0026#34;) Source(-1 to 1) .via(possibleDivisionByZero) .runWith(Sink.foreach(println)) However some errors are recoverable, in this case we provide a PartialFunction[Throwable, T]. It will terminate the stream graciously passing the resulting value as the final value.\nimport $ivy.`com.typesafe.akka::akka-actor:2.6.3` import $ivy.`com.typesafe.akka::akka-stream:2.6.3` import akka.NotUsed import akka.actor.ActorSystem import akka.stream.{ActorAttributes, Supervision} import akka.stream.scaladsl.{Flow, Sink, Source} import java.lang.ArithmeticException val possibleDivisionByZero = Flow[Int].map(i =\u0026gt; 100/i) .withAttributes( ActorAttributes.supervisionStrategy(decider) ) .recover { case _: ArithmeticException =\u0026gt; 0 } implicit val system = ActorSystem(\u0026#34;QuickStart\u0026#34;) Source(-1 to 1) .via(possibleDivisionByZero) .runWith(Sink.foreach(println)) Graphs #  Introduces Junctions which take multiple inputs and multiple outputs. Basic ones are:\n Fan-in: N inputs + 1 output. See operators . Fan-out: 1 input + N outputs. See operators .  For example, using Fan-in we can randombly select one of the inputs, give preference or merely zip them. Then, using Fan-out, we can broadcast the values or unzip to create two individual streams.\n We can use GraphDSL to easily connect them visually (documentation ). E.g.:\nimport $ivy.`com.typesafe.akka::akka-actor:2.6.3` import $ivy.`com.typesafe.akka::akka-stream:2.6.3` import akka.NotUsed import akka.actor.ActorSystem import akka.stream.{ActorAttributes, ClosedShape, Supervision} import akka.stream.scaladsl.{Broadcast, Flow, Merge, RunnableGraph, Sink, Source, GraphDSL} implicit val system = ActorSystem(\u0026#34;QuickStart\u0026#34;) RunnableGraph.fromGraph(GraphDSL.create() { implicit builder: GraphDSL.Builder[NotUsed] =\u0026gt; import GraphDSL.Implicits._ val in = Source(1 to 10) val out = Sink.foreach(println) val bcast = builder.add(Broadcast[Int](2)) val merge = builder.add(Merge[Int](2)) val f1, f2, f3, f4 = Flow[Int].map(_ + 10) in ~\u0026gt; f1 ~\u0026gt; bcast ~\u0026gt; f2 ~\u0026gt; merge ~\u0026gt; f3 ~\u0026gt; out bcast ~\u0026gt; f4 ~\u0026gt; merge ClosedShape // Indicates no open inputs or outputs, this means that the graph is runnable. The opposite means that it is a partial graph. }).run() It is also possible to build partial graphs, or Shapes. There are already some built-in:\n Linear Shapes (\u0026lt;Source|Flow|Sink\u0026gt;Shape) Junction Shapes with the same input/output types (UniformFan\u0026lt;In|Out\u0026gt;Shape). Junction Shapes with different inputs/outputs types (Fan\u0026lt;In|Out\u0026gt;Shape\u0026lt;arity\u0026gt;).  Simpler graphs can be done using, for example, the simpler Sink.combine API.\nFusion #  By default, Akka \u0026ldquo;fuses\u0026rdquo; all stages onto a single syncronous one to run on a single actor (auto-fusing can be disabled) but this limits the benefits we are looking for.\n In order to add a asyncronous boundary, we just need to add async which disables fusing for that stage, which means that we are adding an additional overhead (Actors, mailboxes and buffers). Its benefits largely depends on the use-case. A good principle is\n  Insert an async boundary to bisect the stream into two subsections of roughly equal processing time. We insert an async boundary to bisect the stream into two subsections of roughly equal processing time.   In other words, check at the current pipeline where the stages can be split so that they can be performed in paralell and joined almost at the same time. This implies looking at Telemetry and verify which stages can be processed in paralell given the graph we have.\n","preview":"Actor Streams leverage the actor system to consume streams of data. In fact, each element …"},{"id":8,"href":"/knowledge-base/notes/akka_testing/","title":"Akka testing","tags":["akka"],"content":"How to test actors in Akka.\nTest Actor Ref #  Synchronous and ideal for white-box testing as we can change the internal state.\nTest Probes #  Black-box testing and then verify if specific messages were sent (and other assertions).\n(black box is always better :) )\n","preview":"How to test actors in Akka.\nTest Actor Ref #  Synchronous and ideal for white-box testing …"},{"id":9,"href":"/knowledge-base/notes/amdah_s_law/","title":"Amdah'S law","tags":["concurrency"],"content":"In short, contention limits paralelization.\nDefines the maximum improvement gaines by parallel procesing. Improvements from paralelization are limited to the code that can be paralelized. Contention limits such paralism reducing the advantages of the improvements. Does not matter as long as the contention exist.\n","preview":"In short, contention limits paralelization.\nDefines the maximum improvement gaines by …"},{"id":10,"href":"/knowledge-base/notes/base_transaction/","title":"Base transaction","tags":["system-design"],"content":"Sometimes ACID transactions are not possible in some cases, e.g., microservices. As alternative, we use BASE transactions:\n Basically Available Soft State Eventual Consistency  The difference from ACID transactions it that they cannot be rolledback easily. To rollback, a compensating action is needed to revert to the original state. Saga manages these kind of transactions and are often used to manage different aggregate roots (see Domain Driven Design ).\n","preview":"Sometimes ACID transactions are not possible in some cases, e.g., microservices. As …"},{"id":11,"href":"/knowledge-base/notes/bug-management/","title":"Bug management","tags":["monitoring"],"content":"Some personal notes on simplifying the process so that one can focus on getting back to the other tasks at hand.\nOn Reporting Bugs #   Focus on the impact for the client. You do not need to debug right away. You do not need to establish the timeline - The report can solely include the context. The person assigned to the issue will pick on the context provided in the ticket and explore.  ","preview":"Some personal notes on simplifying the process so that one can focus on getting back to …"},{"id":12,"href":"/knowledge-base/notes/command_query_responsibility_segregation/","title":"Command query responsibility segregation ( cqrs)","tags":["system-design"],"content":"Command Query Responsibility Segregation (CQRS) is a architectural pattern that aims to split applications between two models: Command Model (writes) and Query Model (reads). Some use cases:\n Auditing (e.g., banking, accounting) High Scalability High Resiliency  This separation allows both concerns to evolve separately depending on the requirements. For example, some Aggregate Roots are a better fit for write models but do not fit other read models.\nThis pattern is often combined with Event Sourcing (ES) .\nEventually consistent by design which always present but now is explicit about it.\nHow: #  Application is split between two models:\n Command Model: Handles requests to change the state of the application and decide its side-effects, e.g., events or new commands. Query Model (or View Model or Projection): Focus on data and not on behavior and are modelled to satisfy a very specific need, therefore it is usual having multiple of them.   API -Queries-\u0026gt; Read Model \u0026lt;- Data Store API -Commands-\u0026gt; Write Model -\u0026gt; Data Store Separate process consumes the written events and persists on a denormalized event store, which is called Projection used by the read model like so:\nAPI -Queries-\u0026gt; Read Model \u0026lt;- Data Store API -Commands-\u0026gt; Write Model -\u0026gt; Data Store Data Store -Events-\u0026gt; Denormalized Data Store (with Projections) The Data Store is usually denormalized to make sure that queries are faster.\nSummary #   Write model are optimized for writes Read models are optimized for reads Read and write models are decoupled which implies that they may use different data stores. New Projections are easy in CQRD/ES. New projections are retroactive because we have the fully history.  Models as Microservices #   Write Model can become a microservice Read Model can become a microservice  This assumes that it uses different databases (as expected from proper microservices). Otherwise the database may become the bottleneck.\nBetter yet: Each projection in its own Microservice. \\[\\]\\[\\]\\[\\] and maintentance.\nConsistency #  Simple (without ES) has the same consistency as non-CQRS systems. CQRD/ES can have different consistency models for the read or the write models.\nWrite Model #  Strong is often important here because we want that those write be based on the current state. This consistency is usually implemented through locks or sharding in a more reactive way.\nRead Model #  Pure reads are never consistent as they are often working with stale data. These reads do not need strong consistency.\nAvailability #  Write Model #  Due to the higher consistency, availability is lower.\nRead Model #  Due to the eventual consistency, we can leverage technicques to increase availability.\nCost #  CQRD/ES is often criticized for being more complex but it can be simpler. Without this, models are more bloated, complex and rigid. CQRD allow smaller models that are easier to modify and understand. Eventual consistency in CQRS can be isolated to where it is necessary.\n More databases to maintain More classes/objects to maintain Support older versions can be challenging Additinonal storage Data duplication may result in desyncs that often solved by rebuilding project - Question: when ? Do we have monitoring over this? Can this be automatic? How often does this occur? UI must be designed to be eventually consistent (which was always there in the past, it is now explicit)  ","preview":"Command Query Responsibility Segregation (CQRS) is a architectural pattern that aims to …"},{"id":13,"href":"/knowledge-base/notes/command_sourcing/","title":"Command sourcing","tags":["system-design"],"content":"Command Sourcing #  Similar to Event Sourcing (ES) but persists commands as opposed to events so:\n  Issue command\n  Persist command\n  Run asyncronous the command\n  They should be idempotent as they run multiple times (e.g., failures).\n  Must be validated so that they do not become stuck in the queue forever.\n  Bad: The sender might not be notified if the command fails due to the decouple nature.\n  ","preview":"Command Sourcing #  Similar to Event Sourcing (ES) but persists commands as opposed to …"},{"id":14,"href":"/knowledge-base/notes/consistency_and_availability/","title":"Consistency and availability","tags":["system-design"],"content":"Scalability #  It can meets increases in demand while remaining responsive.\nThis is different from performance. Performance optimizes response time (latency) while scalability optimizes ability to handle load. Requests per second actually measures both but we do not know which aspect was improved.\nNote Scalability is not the number of requests qwe can handle a in a given period of time (req/sec) but he number of requests itself (load).\nIf x axis is number of requests (Load) and y axis is response time. Improving performance leads to decrease in the y axis. Improving scalability means a shift on the x axis meaning that we can handle more requests with the same response time.\nStill confused as if I improve performance I should free up resources to handle more requests..\nReactive Microservices focus on improving scalability.\nConsistency: #  All members of the system have the same view or state. This does not factors time.\nEventual Consistency #  Guarantees that, in the absence of new updates, all accesses to a specific piece of data will eventually return the most recent data.\nDifferent forms:\n Eventual Consistency Causal Consistency Sequential Conssitency others  E.g., Source control are eventually consistent. All the code reading is potentially out-of-date and a merge operations is relied upon to bring the local state back to speed.\nTODO: Check each one #  Strong Consistency #  An update to a piece of data needs agreement from all nodes before it becomes visible.\nPhysically it is impossible therefore we simulate: Locks that introduce overhead in the form of contention. Now it becomes a single resource which eliminates the distributed nature. Tying the distributed problem to a non-distributed resource.\nTraditionally, monoliths are based around strong consistency.\nEffects of contention #  Definition: Any two things that contend for a single limited resource and only one will win and the other will be forced to wait.\nAs the number of resources disputing for the resource, more time time it will take to finally free up the resources.\nSee:\n Amdah\u0026rsquo;s Law.  Gunther\u0026rsquo;s Universal Scalability Law      Coherence Delay\nDefinition: Time it takes for synchronization to complete on a distributed systems - My definition following below notes:\nSyncronization is done using crosstalk or gossip - Each system sends messages to each other node informing of any state changes. The time it takes for the cynscronization to complete is called Coeherency Delay.\nIncreasing the number of nodes increases the delay.\n     Laws of scalability\nBoth these laws demonstrate that linear scalability is almost always unachivable. Such is only possible if the system lieve in total isolation.\n     Reactive Systems\nReduce contention by:\n Isolating locks Eliminating transactions Avoiding blocking operations  Mitigate coherency delays by:\n Embracing Eventual Consistency Building in Autonmy  This allows for higher scalability as we reduce or eliminate these factors.\n  CAP Theorem #  States that a distributed system cannnot provide more than than two of the following:\n Consistency Availability Partition Tolerance  One has to pick one of the following combinations:\n (CP) Consistent and Partition Tolerance (AP) Available and Partition Tolerance.  In practice, they may claim CP/AP except for some edge-cases. It is a balance.\nPartition Tolerance #  The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network.\nThey can occur due to:\n Problems in the network. When a node goes down.  May be short or long lived.\nTwo options:\n (AP) Sacrifice Consistency: Allow writes to both sides of the partition. This require merging the data in order to restore consistency. (CP) Sacrifice Availability: Disabling or terminating on side of the partitions. During this, some or all of your system will be unavailable.  Sharding as a way to have strong consistency #  Limit the scope of the contention and reduce crosstalk. Is applied within the application. It is not the same type of sharding used in some databases, the technique is similar though.\nAllows strong consistency.\nPartitions entities (or Actors) in the domain according to their id.\nGroups of entities are called a shard and each entity only exists in one shard.\nEach shard exists in only one location. This fact eliminates the distributed systems problem.\nThe entity acts as a consistency boundary.\nIn order for this to work, we need to have a coordinator that ensures that traffic for a particular entity is routed to the correct location. The coordinator uses the ID to calculate the appropriate shard.\nAggregate Roots are good candidate for sharding.\nIt is important to have a balanced shards and that requires a good sharding key - UUIDs or hashcodes. Poor key selections will result in hotspots.\nRule of thumb: 10x as many shards as nodes.\nAkka provides this as a means to distribute actors across a cCLuster in a shared setup. Lagom persistent entities levarage akka cluster sharding to distribute the entities across the cluster.\nWhat about resharding? when a system goes down\u0026hellip;\nSharding allows a great caching solution as:\n We can store the cache results after writing to the database Databases is effectively write-only which can speed up things We only consult the cache during reads. Begs the question: How many items and what is the TTL? Well.. it for certain reduces the read on the DB but that is not forever unless we have infinite memory.  Effects #   Does not eliminate contention. It solely isolates to a single entity. The router/coordinator represents a source of contention as well. A shareded system minimizes contention by:  Limiting the amounf of work the router/coordinator performs - By storing where the shard is after asking the coordinator - How to invalidate that cache due to failures? Isolates contention to individual entities    Scalability is doen by distributing the shards over mode machines. Strong consistency is achiaved by isolating operations to a specific entity. Careful choice of shard keys is important to maintain a good scalability.\nFailure #  Sharding sacrifices availability. Once a shard goes down, there will be a period of time where it is unavailable and wil migrate to another node eventually.\nCRDTs provide a availability solution based on async replication #  Conflict-free Replicated Data\nOn the application level.\nHighly available and eventually consistent.\nSpecially designed data type.\nUpdates are applied on one replica and then copied async.\nUdpdates are merged to determine the final state.\nTwo types:\n CvRDT - Convergent Replicated Data Type copy state between replicas. Requires a merge operation that understands how to combine two states. These operations must be: commutative, associative and idempotent. CmRDT - Commutative Replicated Data Types. These copy operations isntead of state.  ","preview":"Scalability #  It can meets increases in demand while remaining responsive.\nThis is …"},{"id":15,"href":"/knowledge-base/notes/coursera/","title":"Coursera","tags":["learning"],"content":"Interested in:\n https://www.coursera.org/learn/learning-how-to-learn  ","preview":"Interested in:\n https://www.coursera.org/learn/learning-how-to-learn  "},{"id":16,"href":"/knowledge-base/notes/databases_overview/","title":"Databases overview","tags":["databases"],"content":"https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78\n","preview":"https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f …"},{"id":17,"href":"/knowledge-base/notes/digital_garden/","title":"Digital garden","tags":["note","zettelkasten"],"content":"Digital gardening is a different take to blogging as it shifts the focus of publishing content to a public to the writing.\nLike gardening, digital gardens start with raw notes and, as we tend to it, connections will start to emerge. But it is personal and informal. There is no pressure on having complete thoughts. You can always circle back to the notes you made in the past and refine them.\n// TODO: Note on how entropy grows and correlate with gardening. See https://joelhooks.com/digital-garden\nVisualizing Digital Gardens #  Visualizing your notes and how they connect with one another is a great way to have a bird-eyes view on how you perceive a topic and it can even help you maturing what you know about it. For example, this This digital garden can be visualized and explored through the following graph:\n   History #  See Maggie Appleton\u0026rsquo;s excellent article .\n","preview":"Digital gardening is a different take to blogging as it shifts the focus of publishing …"},{"id":18,"href":"/knowledge-base/notes/domain_driven_design/","title":"Domain driven design","tags":["system-design","lightbend"],"content":"Approach on Software Development focused on the design of a shared Model understood by domain experts and by who implements it. Being a model, it means that it can be implemented in different ways, from diagrams to software.\nConcepts #   Context: The setting a word appers determines its meaning. Domain: Sphere of knowledge, influence or activity (aka area). Model: System of abstractions that describes a part of the domain. Ubiquitous Language: Common language between domain experts and developers. Aggregate: Entity or group of entities bounded a root entity (Aggregate Root) that are always consistent (within a ACID transaction). Serves as a building block to implement Command within CQRS .  Strategies #  On complex domains, it is hard to have a single unified model, therefore the model is usually split into multiple ones. Follows some strategies used to maintain integrity.\nBounded Context #  Concepts may change from one context to another. It is important to be explicit about it:\n Explicitely define the context where the model applies. Explicitely set the boundaries, e.g., team organization, code-bases, database schemas. Keep the model strictly consistent within these bounds.  Maintain these boundaries strong allows smoother workflows.\nTypically Microservices are build around these contexts. Follows some tips to define them:\n Define use-cases. Look how different groups of people interact with a given entity. Look for variations of the ubiquitous language as they may suggest a new context. Look for variations where informations starts to become or relevant or irrelevant.  Anti-Corruption Layer #  Solves issues where coupling starts. This layer leaves right next to the bounded context to avoid leaking info from/to that bounded context.\nTypical example: Let\u0026rsquo;s put everything in the same bag.\nWhat happens is that we have a layer responsible for translating similar concepts from one bounded context to another.\nHow to implement: Abstract interface as it represents the contract in its purest way without compromising - Sometimes abstractions are too much indirection but do understand. :shrug:\n This is also useful for legacy systems. In this case a Anti Corruption Layer would be preferable on either end.\nContext Map #  Are a way of of visualizating Bounded contexts and the relationships between them.\n  Arrows represent dependencies. Lines may be labelled to indicate the nature of the relationship.  TODO Discovery Process using Event Storming #  Types of Domain Activities #   Command: A request yet to happen and can be rejected. Usually delivered to a specific destination and changes the state of the domain. Events: Action that happened in the past. Hence they can not be rejected. Often broadcast to many destinations. Record a change to the state of the domain, often the result of a command (what are the alternatives?). Query: Requestfor information about the domain. Usually delievered to a specific destination. Do not change the state of the domain.  All of these represent the types of messages in a Reactive System and form the API for a Bounded Context.\nDomain Objects #   Value objects: Defined by its attribute. Immuatable. Messages in Reactive Systems are implemented as Value Objects. Entity: Defined by an unique identity. The fields may change but not its identitity. Are the source of truth - Actors in Akka or Entitities in Lagom  Monolith - Aggregate: Collection of domain objects bound to a root entity:\n Example: Person (Aggregate Root), Name (Aggregate), Address (Aggregate). Transactions should not span multiple aggregate roots. The Aggregate Root may change between bounded contexts. Aggregate Root == Root Entity. Good cadnidates for distribution in Reactive Systems. Question: How to determine?  Is the entity involved in more operations in the same bounded context? Does it make sense deleting other entities when this one is deleted? Will a single transaction span multiple entities?    Domain Abstractiosn #  Services #  Busines Logic encapsulated. Should be stateless otherwise they become an entity or a value object.\nShould be fairly thin.\nFactories #  Constructing domain object may not be trivial as they may have to access external resources (DBs, files, REst APIs, etc).\nRepositories #  Similar to factories but used to get or modify existing objects. They work often over databases but can work with files or Rest Apis (I actually prefer \u0026ldquo;Gateways\u0026rdquo; for Rest APIs).\nNote: Factories and Repositories can be the same in practice.\nHexagonal Architecutre #  Is not directly related with domain driven design but is very compatible.\nDomain is at the core and is at teh center becoming the architectural focus. Then there are ports to communicate with the domain exposed as API for the domain. INfrastructure contains adapters that map to the ports.\nLike an Onion\n Domain  API - The ports Infrastructure - Adapts incoming and outgoing traffic in to the ports.    Outer layers depends on inner layers. And inner layers have no knowledge of other layers.\n:thinking: This does not seem different from the typical layered design with DB -\u0026gt; Services -\u0026gt; API\nEnsures proper spearation of infrastructure from domain.\nThese layers may be modelled through packages or projects. Details are not important. The important thing is to make the domain portable.\nWould like a concrete example on how it really differents from the N tiered design.\n","preview":"Approach on Software Development focused on the design of a shared Model understood by …"},{"id":19,"href":"/knowledge-base/notes/emacs/","title":"Emacs","tags":["editors"],"content":"Org-Mode #  Org-protocol is cool and opens possibilities (like there weren\u0026rsquo;t enoguh :overwhelmed:):\n https://orgmode.org/worg/org-contrib/org-protocol.html#orgheadline8  Configuring #  Literate config is a thing but unfortunately as far as I researched it has to be contained in a single file. I rather have separate files per use-case.\nCool reference links:\n https://tecosaur.github.io/emacs-config/config.html http://doc.norang.ca/org-mode.html:w https://github.com/jethrokuan/dots/blob/0064ea2aab667f115a14ce48292731db46302c53/.doom.d/config.el#L495 https://github.com/nmartin84/.doom.d#orgb81fe7f https://github.com/howardabrams/dot-files/blob/master/emacs-org.org  ","preview":"Org-Mode #  Org-protocol is cool and opens possibilities (like there weren\u0026rsquo;t enoguh …"},{"id":20,"href":"/knowledge-base/notes/event_sourcing_es_es/","title":"Event sourcing ( es)","tags":["system-design"],"content":"Representing the application\u0026rsquo;s state through the history of events that have happened in the past. Use cases:\n Audit Logs: Build tailored reports from the event stream. Analytics: Extract behavior from the event stream. Temporal Reports: Build the timeline that led to a certain state.  This is the opposite of regular applications where the final state is stored. In Event Sourced applications, the final state is called Materialized State. Whenever we need to obtain the current state we replay the logs until we reach the current state without replaying the side-effects. It is possible to keep both representation but that is complex and may become out-of-sync. Moreover, append-only operations are more efficient in databases.\nEvent1 -\u0026gt; Event2 -\u0026gt; Event3 ----\u0026gt; Materialized State As events become the source of truth, it is of utmost importance that they are immutables.\nOptimization through snapshots #  What happens if the list of events is too large? Solution: Ocasionally persist a snapshot and we replay the events from that point on (issue: We may receive an event out-of-order, invalidating the snapshot).\nVersioning #  Issue when we have to change the event\u0026rsquo;s schema. This leads to ModelV1, ModelV2, etc. This requires supporting all versions. And requires flexibile formats: JSON, ProtoBuf or AkkA Event Adapters in the lightbend ecosystem that is between the system and the DB translating the V1, V2, VN to the corresponding and unique Domain entity.\nProblems #  When we need to perform queries that involve several aggregate roots. Because entities need to be rebuilt from events everytime they are visited.\n The model used to persist are not compatible with the model required during queries.\n See Command Query Responsibility Segregation .\n","preview":"Representing the application\u0026rsquo;s state through the history of events that have …"},{"id":21,"href":"/knowledge-base/notes/git/","title":"Git","tags":["snippets"],"content":"Get Previous Tag #  $ git for-each-ref --sort=creatordate --format '%(refname)' refs/tags | sed 's#^refs/tags/##' | tail -n 2 | head -n 1 ","preview":"Get Previous Tag #  $ git for-each-ref --sort=creatordate --format '%(refname)' refs/tags …"},{"id":22,"href":"/knowledge-base/notes/gossip_protocol/","title":"Gossip protocol","tags":["protocols"],"content":"Some of these text may be interwined with specifities of the Gossip Protocol within Akka .\nContext: TODO\nAt a regular interval, each member sends their view of the cluster state to a random node, including:\n The status of each member If each member has seen this version of the cluster state.  Eventually consistent as after some time (aka convergence), all nodes will share the same state. Each node decides if it has reached convergence.\nAfter it, each node will have the same ordered set of nodes:\n Node 1 Node 2 Node 3  Then the first eligible node, will become the leader and will perform leader\u0026rsquo;s duties such as notify member state changes\nNote: that each leader may change after each round if the cluster membership has changed.\nMember Lifecycle - Happy Path #  Joining -\u0026gt; Up -\u0026gt; Leaving -\u0026gt; Exiting -\u0026gt; Removed (tombstoned) In this case:\n Once all nodes know that all nodes know that a new node is \u0026ldquo;Joining\u0026rdquo;, it marks it as \u0026ldquo;Up\u0026rdquo;. A leaving node sets itself as Leaving, then once all nodes know it, the leader marks it as Exiting. Once all leaders know that a node is Exiting, the leader sets the node as Removed.  Member Lifecycle - Unhappy Path #  Joining -\u0026gt; Up -\u0026gt; Leaving -\u0026gt; Exiting -\u0026gt; Removed (tombstoned) | | | | /|\\ |--disconnected--------------| | \\|/ | Unreachable (F) ----\u0026gt; Down (F) ------------ When a new member is disconnected, it is marked as Unreachable until it recovers. If it is permanent, it is flagged as Down and it moves to state Removed and can never come back.\nEach node is monitored for failures by at most 5 nodes using Heartbeat . Once a member is deemed unreachable, then than information will be gossiped to the cluster. The member will remain like that until all nodes flag the node as Reachabe again.\nReasons:\n Crashes Heavy load Network Partition or failure  Impact of Unreachable Nodes #  Convergence will not be possible therefore no leaders will be elected therefore new nodes cannot fully join the cluster. This will happen until the node is marked as Down (and then Removed).\nIn this moment, potential new members will have a state WeaklyUpMember, which can transition to Up once convergence is complete. Note that, this member can be used by applications but should not if consistency is important (Cluster Shards or Cluster Singletons).\nTODO #   Check if Heartbeat and Gossip Protocol are always interwined.  ","preview":"Some of these text may be interwined with specifities of the Gossip Protocol within Akka . …"},{"id":23,"href":"/knowledge-base/notes/gpg/","title":"Gpg","tags":["security","signing"],"content":"Automatically export public key to a server:\n$ gpg --keyserver pgp.mit.edu --send-keys \u0026lt;KEY\u0026gt; ","preview":"Automatically export public key to a server:\n$ gpg --keyserver pgp.mit.edu --send-keys …"},{"id":24,"href":"/knowledge-base/notes/gunther_s_universal_scalability_law/","title":"Gunther'S universal scalability law","tags":["concurrency"],"content":"Increasing concurrency can cause negative resutrns due to contention and coherency delay.\nPicks from Amdah\u0026rsquo;s Law . In addition to contention, it accounts for coeherency delay.\nAs the system scales up, the cost to coordinate between nodes exceeds any benefits.\n","preview":"Increasing concurrency can cause negative resutrns due to contention and coherency delay. …"},{"id":25,"href":"/knowledge-base/notes/hands_on_scala_programming/","title":"Hands on scala programming","tags":["scala"],"content":"Follows my notes on the Haoyi Li.\u0026rsquo;s book: \u0026ldquo;Hands-on Scala Programming\u0026rdquo; (https://www.handsonscala.com/).\nAs an experiment, I coding directly in the org-mode file using Babel to execute the Scala blocks:\nTODO Some introduction #   Point to dotfiles Point to the Babel package  Notes #   Lack of auto-complete when writing here. Compilation errors are hard to track. Workaround is to open a separate buffer with the ammonite REPL console. For sure will have to move to a dedicated project once I use Mill. Or maybe not.  Exercises 3.5 #  3.63 #   “Write a short program that prints each number from 1 to 100 on a new line.\nFor each multiple of 3, print \u0026ldquo;Fizz\u0026rdquo; instead of the number.\nFor each multiple of 5, print \u0026ldquo;Buzz\u0026rdquo; instead of the number.\nFor numbers which are multiples of both 3 and 5, print \u0026ldquo;FizzBuzz\u0026rdquo; instead of the number.”\n   \u0026ldquo;Define a def flexibleFizzBuzz method that takes a String =\u0026gt; Unit callback function as its argument, and allows the caller to decide what they want to do with the output. The caller can choose to ignore the output, println the output directly, or store the output in a previously-allocated array they already have handy.”\n def fizzbuzz(n: Int): String = if (n % 3 == 0 \u0026amp;\u0026amp; n % 5 == 0) \u0026#34;FizzBuzz\u0026#34; else if (n % 3 == 0) \u0026#34;Fizz\u0026#34; else if (n % 5 == 0) \u0026#34;Buzz\u0026#34; else n.toString() @main def main() { (1 to 100) .map(fizzbuzz) .foreach(println) } 3.64 #   “Write a recursive method printMessages that can receive an array of Msg class instances, each with an optional parent ID, and use it to print out a threaded fashion. That means that child messages are print out indented underneath their parents, and the nesting can be arbitrarily deep.”\n  I assume that the items are ordered by their identifier which is incremental with each new message.   class Msg(val id: Int, val parent: Option[Int], val txt: String) def printMessages(messages: Array[Msg]) { val padding = 2 def printMessages(index: Int, msgLevel: Map[Int, Int]) { if (index \u0026lt; messages.length) { val message = messages(index) val level = message.parent match { case Some(parent) =\u0026gt; msgLevel.getOrElse(parent, 0) + 1 case None =\u0026gt; 0 } println(\u0026#34; \u0026#34; * level * padding + message.txt) printMessages(index + 1, msgLevel + (message.id -\u0026gt; level)) } } printMessages(index = 0, msgLevel = Map()) } printMessages(Array( new Msg(0, None, \u0026#34;Hello\u0026#34;), new Msg(1, Some(0), \u0026#34;World\u0026#34;), new Msg(2, None, \u0026#34;I am Cow\u0026#34;), new Msg(3, Some(2), \u0026#34;Hear me moo\u0026#34;), new Msg(4, Some(2), \u0026#34;Here I stand\u0026#34;), new Msg(5, Some(2), \u0026#34;I am Cow\u0026#34;), new Msg(6, Some(5), \u0026#34;Here me moo, moo\u0026#34;) )) ","preview":"Follows my notes on the Haoyi Li.\u0026rsquo;s book: \u0026ldquo;Hands-on Scala Programming\u0026rdquo; …"},{"id":26,"href":"/knowledge-base/notes/heartbeat/","title":"Heartbeat","tags":["protocols"],"content":"In order to detect failures, systems communicate with one another to verify communication. If the communication is deemed broken, then the system may be considered as Unreachable depending on he heartbeat history and how the Failure Detection is configured. I.e., a single heartbeat does not mean that the member is Unreachable.\n","preview":"In order to detect failures, systems communicate with one another to verify communication. …"},{"id":27,"href":"/knowledge-base/notes/jackson/","title":"Jackson","tags":["jvm","json"],"content":"Jackson is a library to serialize and deserialize JSON in the JVM world.\nSane Settings #  After working a while with this I want to register these sane defaults:\nconfigure(DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES, true) configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false) FAIL_ON_NULL_FOR_PRIMITIVES #  Setting FAIL_ON_NULL_FOR_PRIMITIVES forces clients to explicitely provide all values including primitives. Consider the following POJO:\ndata class Foo(bar: Boolean) Without the setting, a payload such as { } would render ~Foo(bar=false)~ despite the lack of default value. Given this, to guarantee consistency between the source-code and the external contract, I advise enabling this.\nFAIL_ON_UNKNOWN_PROPERTIES #  Setting FAIL_ON_UNKNOWN_PROPERTIES is useful when working on two systems in paralel. Settings this to false enables clients to send fields to the server that are not yet supported but will be. The alternative would be:\n Server includes those fields as optional fields (to avoid breaking current clients). Server roll-out. Clients update their HTTP clients to include the new fields. Once all-known clients support the new fields, make the same fields mandatory.  By setting this to true, this whole orchestration is not required.\nTODO Sub-Types #  Consider the following example that attempts to model a DSL that supports + and - operations.\nsealed class Expression data class Sum(val a: Int, val b: Int): Expression() data class Sub(val a: Int, val b: Int): Expression() data class Request( @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.EXTERNAL_PROPERTY, property = \u0026#34;type\u0026#34;, visible = true) @JsonSubTypes( JsonSubTypes.Type(value = Sum::class, name = \u0026#34;+\u0026#34;) JsonSubTypes.Type(value = Sub::class, name = \u0026#34;-\u0026#34;) ) val operation: Expression ) In short, this is saying if the JSON contains a field name type with value + it would deserialize to Sum and to Sub if type had value -. I.e., a sum operation between 1 and 2 would require the following JSON payload:\n{ \u0026#34;type\u0026#34;: \u0026#34;+\u0026#34;, \u0026#34;operation\u0026#34;: { \u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2 } } ","preview":"Jackson is a library to serialize and deserialize JSON in the JVM world.\nSane Settings # …"},{"id":28,"href":"/knowledge-base/notes/message_driven_architecture/","title":"Message driven architecture","tags":["system-design"],"content":"Asyncronous and non-blocking. The sender does not actively wait for a response.\nAdvantages:\n Resources are freed immediatly. Reduced contention Messages can be queued for deleivery in case the receiver\u0026rsquo;s is offline. Provides a higher level of reliability.  Disavantages:\n Make transactions more difficult. How to manage long running transactions that span multiple microservices. Holding transactions open for long periods result in slow, brittle systems.  The role of syncronous messags:\n Can you acknowledge the message but process it asyncronously? The need for syncronous messages should be driven by domain requirements rather than technical convenience.  Sagas #  Represent long running transaction. Multiple requests are managed by a Saga.\nIndividual requests are run in sequence or in paralel.\nHow:\n Each request is paired with a compensating action. If any requests fails, compnesation actions are executed for all completed steps. Then the saga is completed with a failure. If compensation actions fails, then it is retried. This requires idempotency.  Timeouts in distributed systems:\n  Eitehr the request failed.\n  Either it was successful but the reply failed.\n  The request is still queued and may success or fail eventually.\nCompensating actions != Rollbacks.\nRollback: implies the transaction has not completed which removes the evidence of the transaction. Compensation: Applied on top of a previously completed action. Evidence of the orignal action remains.\n  Disavantages:\n Coupled to the failures unless we can move regardless of that. Saga are often implemented using ackka actors and represented via Finite State Machine.  Two General Problem #  Illustrate the impossibility of reaching a concensus over an unreliable communication channel.\nDelivery Guarantees #   Exactly Once: Is not possible in the event of a network partition, or lost message. We never guarantee that the message was in-fact sent. Failure requires resending the message which creates potential duplicates. Reuqires storage on both ends: unreliable network (as always); timeouts. At most once - If a failure occur, no retries are done which means no duplications but there may be losses. Requires no storage of messages. At least once - Require a acknoledge and teh sender needs to store state to track whether the messsage was acknowledge. It ahs to be stored in a durable data store.  Exactly once can be simulated using at least once and idempotency.\nAkka: at most once by default. Akka persistence has option to have at least once.\nMessaging Patterns #  Publish Subscribe #  Decoupled. The only coupling is on the message format and possibily the location (e.g., url, exchange on the message broker). Complexity is hard to see as we do not know where the message comes from.\nPoint to point #  Dependencies more clear but coupling is higher. COmplexity is directly observable.\nExamples #  Kafaka, RabbitMQ. Kafka allows point to point and pub/sub and we can even acknowledge once we finish processing.\nAkka: Typically point to point messaging;Persistence Query: Pub/sub Lagom: Point to point communication between services. Messages broker API allow for pub/sub.\n","preview":"Asyncronous and non-blocking. The sender does not actively wait for a response. …"},{"id":29,"href":"/knowledge-base/notes/messaging-systems-overview/","title":"Messaging systems comparison","tags":["messaging"],"content":"Kafka #  Event Streaming, persistent.\nRabbitMQ #  Low latency.\nPulsar #  Made by Apache\nTODO #   Read this comparison: https://www.confluent.io/kafka-vs-pulsar/#:~:text=In%20reality%2C%20Kafka%2C%20RabbitMQ%2C,Pulsar%20sits%20somewhere%20in%20between.  ","preview":"Kafka #  Event Streaming, persistent.\nRabbitMQ #  Low latency.\nPulsar #  Made by Apache …"},{"id":30,"href":"/knowledge-base/notes/microservices/","title":"Microservices","tags":["system-design"],"content":"Subset of Service Oriented Architecture (SOA) where each service is deployed separately:\n Microservices can be physically separated and independently deployed. Each have its own data store. Independent and self governing. Communication is syncronous or asyncronous (e.g., through messaging systems ). Loose coupling between components (more or less by experience but that is design flaw likely :thinking:). Shorter development and release cycles. Each scale independently (either through physical or virtual machines).  Advantages #   Deployed/Scaled as needed. Increase availability due to reduced single point of failures. Better isolation leading to less couling giving flexibility. Supports multiple platforms and languages. More indepentent Shorter cycles  Mention that cross team coordination become less necessary. That is true but often it still is and requires coordination.\nDisavantages #   Require multiple complex deployment nad monitoring approaches. Cross service refactors are more challenging. Require supporting old versions. Organization Change  Designing one #  Business domains are typically large and complicated. Several ideas, actions and rules interact with one another in complex ways. A good approach on understanding such complex systems and how to model them is through a process named Domain Driven Design .\nA correct separation of concerns (i.e., clear and single responsibilities) leads to smoother workflows. Awkarwdness when modelling may indicate lack of understanding of the domain.\nPrinciples of Isolation #  State #  All access must go thorugh the API and there is no backdoors. Allows internal changes without affecting others.\nSpace #  Should not care where the other services are deployed. Allows microservices to be scaled up or down to meet demands. However, if latency is an issue, they should be in the same region.\nTime #  Should not wait for each other which allows more efficient use of resources. Resources can be freed immediatly, rather than waiting for a request to finish.\nBetween microservices we expect eventual consistency. It improves scalability as total consitency requires central coordination which hindes scalability.\nNOTE: This is discussible. However it may be a implementation details.\nFailure #  Isolate failures and should not cause failure on other systems. I.e., it still is responsive to attend other use-cases.\nIsolation Techniques #  Bulkheading #  Failures are isolated to failure zones. Failures in one service will not propagate to other services. Overall the system remains operation but possibly in a degraded state.\nIn practice it means that that systems that depend on the service that is considered a failure zone, will mark that information or service as unavailable. IMO this is tolerable if the service is non-critical.\nCircuit Breaker #  When a service is under stress we do not want to keep on retrying as it may make things worse.\nWay to avoid overloading a service. They qaurantine a failing service so it can fail fast. Allows the failing service to recover in its time before they fail.\nTypes:\n Closed - Normal Operation Open - Fail fast Half Open - Internally after a timeout will let one request to go through and if it fails, it goes back to Open.  Transitions:\n Closed (normal) -\u0026gt; Trip: Open (Fail Fast) Open (Fail fast) -\u0026gt; Attempt Reset: Half Open Half Open -\u0026gt; Trip: Open (fail Fast) Half Open -\u0026gt; Reset: Closed (Normal)  Message Driven Architecture #   Async and non blocking messages allows decoupling both time and failure. System do not depend on the response from on another. If a request to a service fails, the failure will not propagated. The client service isn\u0026rsquo;t waiting for response. It can continue to operate normally.  Autonomy #  Services can operate independently from each other.\nAutonomous services have enough information to resolve conflicts and repair failures. This means that they do not require other services to be operational all the time. Ideally all the time but in reality for a short time it guarantees some level of autonomy.\nBenefits:\n Stronger scalability and availability. Can be scaled indefinetly. Operate independently.  How:\n Async messages. Maintain enough internal state ofr the microservices to function in isolation. Use eventual consistency. Avoid direct, syncronous dependencies on external services.  API Gateway Services #  Microservices can lead to complexity in the API. How can we manage complex APIs that access many microservices?\nA new microservice that is put between the client and the N-services that are required to fulfill that request. This new microservice is responsible for aggregating the responses moving the responsbility from the client itself. This way, the client only needs to manage failures from the gateway.\nThis effectively creates an additional layer of isolation.\n!! This is specially useful for mobile applicatios where we cannot guarantee that the clients will update their app.\n","preview":"Subset of Service Oriented Architecture (SOA) where each service is deployed separately: …"},{"id":31,"href":"/knowledge-base/notes/monolith/","title":"Monolith","tags":["system-design"],"content":"Charateristics #   Deployed as a single unit. No Clear Isolation. Complex Depedencies which in turn makes it hard to understand and modify. Big Bang Style Releases Long Cycle Times Careful releases Scalation is done with multiple copies and uses the database as consistency between them.  Advantages: #   Easy Cross Module Refactor Easier to maitain consistency Single Deploy Process Single thing to monitor Simple Scalability Model  Disavantages: #   Limited by the maximum size of a single phyisical machine. Only scales as the database allows. Components are scaled as a group. Deep coupling. Long Dev Cycle. Lack of reliaability given that one failure may impact the whole monolith.  Tearing it up #  Introduce domain boundaries within the application itself (e.g., libraries). This falls within Service Oriented Architecture (SOA) .\n","preview":"Charateristics #   Deployed as a single unit. No Clear Isolation. Complex Depedencies …"},{"id":32,"href":"/knowledge-base/notes/nix/","title":"Nix","tags":["uncategorized"],"content":"Nix is a declarative language aiming produce reproducible systems. In can be used to produce dotfiles.\nFollows three tools:\n Home Manager : Manages user\u0026rsquo;s home. It can\u0026rsquo;t be used to install fonts for example. Nix Darwin : Manages macOS systems.  The goal is to compose these tools to produce a reproducible generation of one\u0026rsquo;s environment. To aid this, Nix Flakes introduces another layer on top that aims to pin the versions the dependencies through a flake.lock file.\nMy dotfiles leverage Nix.\n","preview":"Nix is a declarative language aiming produce reproducible systems. In can be used to …"},{"id":33,"href":"/knowledge-base/notes/reactive_streams/","title":"Reactive streams","tags":["system-design"],"content":"Components of a Reactive Stream:\n Publisher: Publishes data to stream Subscriber: Consumes data from the stream. Processor: Acts as both a publisher and a subscriber, obeying the contract for each. Subscription: Connects a subscriber to a publisher to initiate a message flow.  Akka Streams is built on these concepts but provides a different API (albeit is possible to bridge).\n","preview":"Components of a Reactive Stream:\n Publisher: Publishes data to stream Subscriber: Consumes …"},{"id":34,"href":"/knowledge-base/notes/reactive_systems/","title":"Reactive systems","tags":["system-design"],"content":"Goal #  Provide an experience that is responsive under all conditions. Note that reactive programming is not the same as reactive systems.\nThis requires:\n Ability to scale from 10 users to million of users. Consume solely the resources required to support the current work-load.  Reactive Principles #  Systems that apply the following principles are considered reactive systems (see more here ).\nResponsive #  Always respond in a timely manner.\nResiliency #  Isolate failures on single components - Similar to how a boat is designed.\nElastic #   Keep responsive specially when the system load changes which provides a more efficient usage of resources. Reduce contentions. Avoid central bottlenecks. Predictive Auto-Scaling policies.  Message Driven #   Losse coupling Isolatation - Kinda disagree with loose coupling, as the systems still will still depend on 3rd party behavior regardless of the communication medium. Resources are only consumed while active  Avoce all, make sure that the systems is not actively waiting so that it can do something else in the mean time which leads to a Async and non-blocking messages.\nGit as example #   Asyncronous and non-blocking as the work is submitted through PR and I can keep on working locally with no interruptions. Message based as it is basically \u0026ldquo;please review this\u0026rdquo;. Resiliency as each user has basically a copy of the whole repository locally. The local machine is isolated from the remote. Elastic has we can have multiple copies and does not cause problemas having that repository sync on that many machines. Responsive.  TODO https://www.lightbend.com/white-papers-and-reports/reactive-programming-versus-reactive-systems #  ","preview":"Goal #  Provide an experience that is responsive under all conditions. Note that reactive …"},{"id":35,"href":"/knowledge-base/notes/saga/","title":"Saga","tags":["system-design"],"content":"Applied in microservices to manage complex workflows that may fail and need to be rolledback.\nThis document requires more details.\n","preview":"Applied in microservices to manage complex workflows that may fail and need to be …"},{"id":36,"href":"/knowledge-base/notes/scala_experiments/","title":"Scala experiments","tags":["scala","data-structures"],"content":"Implementing Tree #  sealed class Tree[T] { def stream(): LazyList[T] = this match { case Leaf(value) =\u0026gt; LazyList(value) case Branch(left, right) =\u0026gt; left.stream() #::: right.stream() } } case class Branch[T](left: Tree[T], right: Tree[T]) extends Tree[T] case class Leaf[T](value: T) extends Tree[T] val tree = Branch( Leaf(1), Branch( Leaf(2), Leaf(3) ) ) val nextInOrder = tree.stream().dropWhile(_ != 2).drop(1).headOption println(nextInOrder) ","preview":"Implementing Tree #  sealed class Tree[T] { def stream(): LazyList[T] = this match { case …"},{"id":37,"href":"/knowledge-base/notes/service_oriented_architecture_soa/","title":"Service oriented architecture ( soa)","tags":["system-design"],"content":"TODO REVIEW THESE NOTES - Architectural Patterns #  As opposed to Monolith , services do not share a database and all access must be done through a API exposed by the service. They may be in the same process (Monolith ) or may be separated (Microservices ). This reduces coupling.\n","preview":"TODO REVIEW THESE NOTES - Architectural Patterns #  As opposed to Monolith , services do …"},{"id":38,"href":"/knowledge-base/notes/sharding_or_partitioning/","title":"Sharding or partitioning","tags":["databases","system-design"],"content":"Technique used by some data stores to reduce contention without sacrificing consistency.\nRecords are distributed across nodes using a Shard Key or a Partition Key that will be used by the Database Router that redirects requests to the correct shard/partition.\nBenefits:\n Contention is isolated to a shard/partition. Given that each shard stores a part of the dataset, it is only handling a small part of the overall load. Improves elasticity.  However, with time the database will still be a bottleneck because there may be too many users acesssing the shards/partitions (sometimes it may be a bad choice of partition key).\n","preview":"Technique used by some data stores to reduce contention without sacrificing consistency. …"},{"id":39,"href":"/knowledge-base/notes/sli_slo/","title":"Sli slo","tags":["system-design"],"content":" Service Level Indicator (SLI): Performance indicator measured as a ratio of two numbers. Service Level Objective (SLO): Defines a target SLI as measurment of the systems\u0026rsquo;s reliability. Service Level Agreement (SLA): Business contract regarding the expected SLO.  It is far productive measuring them as use-histories as it defines the critical paths.\n","preview":"Service Level Indicator (SLI): Performance indicator measured as a ratio of two numbers. …"},{"id":40,"href":"/knowledge-base/notes/web-stack-enties/","title":"Stack web notes","tags":["uncategorized"],"content":"How To Drive Change as a Software Engineer #  Source: https://www.lihaoyi.com/post/HowToDriveChangeasaSoftwareEngineer.html\nThe Dark Side of Events - YouTube #  Source: https://www.youtube.com/watch?v=URYPpY3SgS8\u0026amp;feature=youtu.be\u0026amp;t=1884\nAsk HN: How to Take Good Notes? | Hacker News #  Source: https://news.ycombinator.com/item?id=22473209\nUnlearning toxic behaviors in a code review culture | by Sandya Sankarram | Medium #  Source: https://medium.com/@sandya.sankarram/unlearning-toxic-behaviors-in-a-code-review-culture-b7c295452a3c\nOrg Mode - Organize Your Life In Plain Text! #  Source: http://doc.norang.ca/org-mode.html#GettingOrgModeWithGit\n🧠 Own Your Second Brain: Set Up org-roam on Your Own Machine #  Source: https://www.ianjones.us/own-your-second-brain\nNicolas Mattia – Nix: A Reproducible Setup for Linux and macOS #  Source: https://www.nmattia.com/posts/2018-03-21-nix-reproducible-setup-linux-macos.html\nWriting a book: is it worth it? | Hacker News #  Source: https://news.ycombinator.com/item?id=24628549\nPresets | Starship #  Source: https://starship.rs/presets/#nerd-font-symbols\nMiniflux Hosting #  Source: https://miniflux.app/hosting.html\nRemoteRetro.org | Free. World-class. Agile retrospectives. #  Source: https://remoteretro.org/retros/\nWrite code that is easy to delete, not easy to\u0026hellip; — programming is terrible #  Source: https://programmingisterrible.com/post/139222674273/how-to-write-disposable-code-in-large-systems\nThe Product-Minded Software Engineer - The Pragmatic Engineer #  Source: https://blog.pragmaticengineer.com/the-product-minded-engineer/\nSSH Agent Explained #  Source: https://smallstep.com/blog/ssh-agent-explained/\nRemembering what you Read: Zettelkasten vs P.A.R.A. #  Source: https://www.zainrizvi.io/blog/remembering-what-you-read-zettelkasten-vs-para/\ndegoogle | A huge list of alternatives to Google products. Privacy tips, tricks, and links. #  Source: https://degoogle.jmoore.dev/#useful-links-tools-and-advice\nLPT_LISA - lisa19_maheshwari.pdf #  Source: https://www.usenix.org/sites/default/files/conference/protected-files/lisa19%5Fmaheshwari.pdf\nNo, Microsoft is not rebasing Windows to Linux #  Source: https://boxofcables.dev/no-microsoft-is-not-rebasing-windows-to-linux/\nHow to teach yourself hard things #  Source: https://jvns.ca/blog/2018/09/01/learning-skills-you-can-practice/\nChoose Boring Technology #  Source: http://boringtechnology.club/\n","preview":"How To Drive Change as a Software Engineer #  Source: …"},{"id":41,"href":"/knowledge-base/notes/stateless/","title":"Stateless","tags":["system-design"],"content":" Requests are self-contained and have all the information required to be completed. Requests can be processed on any instance of the application.  Some \u0026ldquo;Stateless\u0026rdquo; systems are not trully stateless as the state is contained in a database:\n Required to have strong consistency (the single source of truth). However this means that the database may become the bottleneck. The database also represents the single point of failure which may lead to an unresponsive systems.  ","preview":"Requests are self-contained and have all the information required to be completed. …"},{"id":42,"href":"/knowledge-base/notes/states-of-data/","title":"States of data","tags":["data"],"content":"States of data:\n At Rest: Data that is not consumed at the time is ingested. It is stored and then consumed later in a batch process. In Transit: Data that is travelling between point A and point B. In Use: Data that is opened for treatment  Encountered several concerns regarding how such data must be handled. Follows a link to be reviewed later:three-states-of-data ","preview":"States of data:\n At Rest: Data that is not consumed at the time is ingested. It is stored …"},{"id":43,"href":"/knowledge-base/notes/technical_writing/","title":"Technical writing","tags":["learning"],"content":"https://developers.google.com/tech-writing/overview\n","preview":"https://developers.google.com/tech-writing/overview"},{"id":44,"href":"/knowledge-base/notes/thinking_tools/","title":"Thinking tools","tags":["work"],"content":"Follows some interesting resources:\n https://untools.co/  ","preview":"Follows some interesting resources:\n https://untools.co/  "},{"id":45,"href":"/knowledge-base/notes/way-of-work/","title":"Way of work","tags":["work"],"content":"It\u0026rsquo;s not about being right nor prove others wrong #   Corollary 1: if it\u0026rsquo;s wrong but it works, then it\u0026rsquo;s not wrong. Corollary 2: if you\u0026rsquo;re right but it doesn\u0026rsquo;t change the outcome, then it doesn\u0026rsquo;t matter. Corollary 3: if you\u0026rsquo;re right, but it doesn\u0026rsquo;t work, then you\u0026rsquo;re wrong. Corollary 4: if you prove someone else wrong, but their answer works and yours doesn\u0026rsquo;t, then they\u0026rsquo;re right and you\u0026rsquo;re wrong. Corollary 5: if you prove someone\u0026rsquo;s solution to be wrong even though it does provide value, then you have not yet provided any value until you propose something better.  Balancing Work/Life #  There is much in life beyond working. Follows an interesting article that contrasts with the tendency:\n https://1x.engineer/  ","preview":"It\u0026rsquo;s not about being right nor prove others wrong #   Corollary 1: if it\u0026rsquo;s …"}]