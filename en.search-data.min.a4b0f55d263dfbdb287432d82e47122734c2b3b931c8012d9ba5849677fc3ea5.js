'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/concurrency/actor-model/','title':"Actor Model",'section':"Concurrency",'content':"Concept from 1973 by Carl Hewitt:\n Actor is the fundamental unit of computation embodying processing, storage, and communication.\n Fundamentals #   All computation occurs inside of the actor. Each actor has an address. Actors may create new actors, send messages to them and changing their own behavior to handle new messages (e.g., change the state).  An actor does not exist isolated, it works in tandem with others and are arranged in hierarquy:\n Actors can split up and delegate tasks to child actors. Child actors are supervised and delegate their failures back to their parent.  Anatomy #  Each actor has:\n An address. An mailbox. An dispatcher.  Messages are sent to an Actor Address which are enqued in the mailbox and only then are dispatched for processing. Note that, only one message is dispatched at a time leading to the ilusion of single thread (there may be multiple actor within the same system, each one handling messages in different threads).\nIt is important that that actors communicate exclusively through messages and do not shared state to ensure Strong Consistency! Moreover messages must be immutable.\n  Reactive Systems #  In the context of Reactive Systems, Actor model is a reactive tool, a paradigm that:\n Actors only communicate through asyncronous messages. Message driven - All communication between actors is done with async non-blocking messages. Abstractions provide elasticity and resiliency.  Akka uses the actor model - https://doc.akka.io/docs/akka/current/typed/guide/actors-intro.html?language=scala\nThe message driven system provides location transparency, i.e., the technique remainins the same regardless of where the actors are. This allows better resiliency and elastic (hmm.. questions on this bit). This is different from \u0026ldquo;Transparent remoting\u0026rdquo; as this hides potential networking issues while making it seem like local calls. Location transparency makes the opposite which is makes local calls seem like remote calls, therefore whoever uses is aware that there are potential failures.\nAkka is an implementation of the Actor Model.\n"});index.add({'id':1,'href':'/concurrency/amdah_s_law/','title':"Amdah's Law",'section':"Concurrency",'content':"In short, contention limits paralelization.\nDefines the maximum improvement gaines by parallel procesing. Improvements from paralelization are limited to the code that can be paralelized. Contention limits such paralism reducing the advantages of the improvements. Does not matter as long as the contention exist.\n"});index.add({'id':2,'href':'/data-processing/apache-spark/','title':"Apache Spark",'section':"Data Processing",'content':"Glossary #   RDD - Resilient Distributed Datasets  "});index.add({'id':3,'href':'/monitoring/bug-management/','title':"Bug Management",'section':"Monitoring",'content':"Some personal notes on simplifying the process so that one can focus on getting back to the other tasks at hand.\nOn Reporting Bugs #   Focus on the impact for the client. You do not need to debug right away. You do not need to establish the timeline - The report can solely include the context. The person assigned to the issue will pick on the context provided in the ticket and explore.  "});index.add({'id':4,'href':'/system-design/command_query_responsibility_segregation/','title':"Command Query Responsibility Segregation (CQRS)",'section':"System Design",'content':"Use Cases #   Auditing (e.g., banking, accounting) High Scalability High Resiliency  Some Aggregate Roots are a better fit for write models but do not fit other read models. Requirements for both models are different.\nWhat #  Command Query Responsibility Segregation (CQRS) aims to separate read models from write models. This is specially problematic when using Event Sourcing. Note that Event Sourcing is not a requirement for CQRS but are often combined.\nEventually consistent by design which always present but now is explicit about it.\nHow: #  API -Queries-\u0026gt; Read Model \u0026lt;- Data Store API -Commands-\u0026gt; Write Model -\u0026gt; Data Store Notes:\n There can be multiple read models to satisfy different use-cases.  Each model is optimized for their purpose.\nSeparate process consumes the written events and persists on a denormalized event store, which is called Projection used by the read model like so:\nAPI -Queries-\u0026gt; Read Model \u0026lt;- Data Store API -Commands-\u0026gt; Write Model -\u0026gt; Data Store Data Store -Events-\u0026gt; Denormalized Data Store (with Projections) Denormalization makes queries fast as there is no need for complex joins or queries.\nSummary #   Flexible model Write model are optimized for writes Read models are optimized for reads Read and write models are decoupled which implies that they may use different data stores. New Projections are easy in CQRD/ES. New projections are retroactive because we have the fully history.  Models as Microservices #   Write Model can become a microservice Read Model can become a microservice  This assumes that it uses different databases (as expected from proper microservices). Otherwise the database may become the bottleneck.\nBetter yet: Each projection in its own Microservice. \\[\\]\\[\\]\\[\\] and maintentance.\nConsistency #  Simple (without ES) has the same consistency as non-CQRS systems. CQRD/ES can have different consistency models for the read or the write models.\nWrite Model #  Strong is often important here because we want that those write be based on the current state. This consistency is usually implemented through locks or sharding in a more reactive way.\nRead Model #  Pure reads are never consistent as they are often working with stale data. These reads do not need strong consistency.\nAvailability #  Write Model #  Due to the higher consistency, availability is lower\nRead Model #  Due to the eventual consistency, we can leverage technicques to increase availability.\nCost #  CQRD/ES is often criticized for being more complex but it can be simpler. Without this, models are more bloated, complex and rigid. CQRD allow smaller models that are easier to modify and understand. Eventual consistency in CQRS can be isolated to where it is necessary.\n More databases to maintain More classes/objects to maintain Support older versions can be challenging Additinoal storage Data duplication may result in desyncs that often solved by rebuilding project - Question: when ? Do we have monitoring over this? Can this be automatic? How often does this occur? UI must be designed to be eventually consistent (which was always there in the past, it is now explicit)  "});index.add({'id':5,'href':'/system-design/command_sourcing/','title':"Command Sourcing",'section':"System Design",'content':"Command Sourcing #  Similar to Event Sourcing but persists commands as opposed to events so:\n  Issue command\n  Persist command\n  Run asyncronous the command\n  They should be idempotent as they run multiple times (e.g., failures).\n  Must be validated so that they do not become stuck in the queue forever.\n  Bad: The sender might not be notified if the command fails due to the decouple nature.\n  "});index.add({'id':6,'href':'/system-design/consistency_and_availability/','title':"Consistency And Availability",'section':"System Design",'content':"Scalability #  It can meets increases in demand while remaining responsive.\nThis is different from performance. Performance optimizes response time (latency) while scalability optimizes ability to handle load. Requests per second actually measures both but we do not know which aspect was improved.\nNote Scalability is not the number of requests qwe can handle a in a given period of time (req/sec) but he number of requests itself (load).\nIf x axis is number of requests (Load) and y axis is response time. Improving performance leads to decrease in the y axis. Improving scalability means a shift on the x axis meaning that we can handle more requests with the same response time.\nStill confused as if I improve performance I should free up resources to handle more requests..\nReactive Microservices focus on improving scalability.\nConsistency: #  All members of the system have the same view or state. This does not factors time.\nEventual Consistency #  Guarantees that, in the absence of new updates, all accesses to a specific piece of data will eventually return the most recent data.\nDifferent forms:\n Eventual Consistency Causal Consistency Sequential Conssitency others  E.g., Source control are eventually consistent. All the code reading is potentially out-of-date and a merge operations is relied upon to bring the local state back to speed.\nTODO: Check each one #  Strong Consistency #  An update to a piece of data needs agreement from all nodes before it becomes visible.\nPhysically it is impossible therefore we simulate: Locks that introduce overhead in the form of contention. Now it becomes a single resource which eliminates the distributed nature. Tying the distributed problem to a non-distributed resource.\nTraditionally, monoliths are based around strong consistency.\nEffects of contention #  Definition: Any two things that contend for a single limited resource and only one will win and the other will be forced to wait.\nAs the number of resources disputing for the resource, more time time it will take to finally free up the resources.\nSee:\n Amdah\u0026rsquo;s Law. Gunther\u0026rsquo;s Universal Scalability Law     Coherence Delay\nDefinition: Time it takes for synchronization to complete on a distributed systems - My definition following below notes:\nSyncronization is done using crosstalk or gossip - Each system sends messages to each other node informing of any state changes. The time it takes for the cynscronization to complete is called Coeherency Delay.\nIncreasing the number of nodes increases the delay.\n     Laws of scalability\nBoth these laws demonstrate that linear scalability is almost always unachivable. Such is only possible if the system lieve in total isolation.\n     Reactive Systems\nReduce contention by:\n Isolating locks Eliminating transactions Avoiding blocking operations  Mitigate coherency delays by:\n Embracing Eventual Consistency Building in Autonmy  This allows for higher scalability as we reduce or eliminate these factors.\n  CAP Theorem #  States that a distributed system cannnot provide more than than two of the following:\n Consistency Availability Partition Tolerance  One has to pick one of the following combinations:\n (CP) Consistent and Partition Tolerance (AP) Available and Partition Tolerance.  In practice, they may claim CP/AP except for some edge-cases. It is a balance.\nPartition Tolerance #  The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network.\nThey can occur due to:\n Problems in the network. When a node goes down.  May be short or long lived.\nTwo options:\n (AP) Sacrifice Consistency: Allow writes to both sides of the partition. This require merging the data in order to restore consistency. (CP) Sacrifice Availability: Disabling or terminating on side of the partitions. During this, some or all of your system will be unavailable.  Sharding as a way to have strong consistency #  Limit the scope of the contention and reduce crosstalk. Is applied within the application. It is not the same type of sharding used in some databases, the technique is similar though.\nAllows strong consistency.\nPartitions entities (or Actors) in the domain according to their id.\nGroups of entities are called a shard and each entity only exists in one shard.\nEach shard exists in only one location. This fact eliminates the distributed systems problem.\nThe entity acts as a consistency boundary.\nIn order for this to work, we need to have a coordinator that ensures that traffic for a particular entity is routed to the correct location. The coordinator uses the ID to calculate the appropriate shard.\nAggregate Roots are good candidate for sharding.\nIt is important to have a balanced shards and that requires a good sharding key - UUIDs or hashcodes. Poor key selections will result in hotspots.\nRule of thumb: 10x as many shards as nodes.\nAkka provides this as a means to distribute actors across a cCLuster in a shared setup. Lagom persistent entities levarage akka cluster sharding to distribute the entities across the cluster.\nWhat about resharding? when a system goes down\u0026hellip;\nSharding allows a great caching solution as:\n We can store the cache results after writing to the database Databases is effectively write-only which can speed up things We only consult the cache during reads. Begs the question: How many items and what is the TTL? Well.. it for certain reduces the read on the DB but that is not forever unless we have infinite memory.  Effects #   Does not eliminate contention. It solely isolates to a single entity. The router/coordinator represents a source of contention as well. A shareded system minimizes contention by:  Limiting the amounf of work the router/coordinator performs - By storing where the shard is after asking the coordinator - How to invalidate that cache due to failures? Isolates contention to individual entities    Scalability is doen by distributing the shards over mode machines. Strong consistency is achiaved by isolating operations to a specific entity. Careful choice of shard keys is important to maintain a good scalability.\nFailure #  Sharding sacrifices availability. Once a shard goes down, there will be a period of time where it is unavailable and wil migrate to another node eventually.\nCRDTs provide a availability solution based on async replication #  Conflict-free Replicated Data\nOn the application level.\nHighly available and eventually consistent.\nSpecially designed data type.\nUpdates are applied on one replica and then copied async.\nUdpdates are merged to determine the final state.\nTwo types:\n CvRDT - Convergent Replicated Data Type copy state between replicas. Requires a merge operation that understands how to combine two states. These operations must be: commutative, associative and idempotent. CmRDT - Commutative Replicated Data Types. These copy operations isntead of state.  "});index.add({'id':7,'href':'/learning/coursera/','title':"Coursera",'section':"Learning",'content':"Interested in:\n https://www.coursera.org/learn/learning-how-to-learn  "});index.add({'id':8,'href':'/work/curriculum/','title':"Curriculum",'section':"Work",'content':"Some reference pages:\n https://www.myperfectresume.com/cv/examples/computer-software/engineer https://business.linkedin.com/talent-solutions/blog/linkedin-best-practices/2016/7-linkedin-profile-summaries-that-we-love-and-how-to-boost-your-own  "});index.add({'id':9,'href':'/system-design/domain_driven_design/','title':"Domain Driven Design",'section':"System Design",'content':"Goal #  Evolving Shared Model understood by domain experts and by who implements it. Being a model, it means that it can be implemented in different ways, from diagrams to software. Regardless of how it is implemented, it must reflect the model.\nUseful for modelling use-cases before attempting to implement it whether in Software or mere Diagrams.\nUbiquitous Languague #  Common language that enables communication between the domain experts and the developers. Terminology in there comes from the domain experts. Software terms should be avoided and any addition must go through domain experts as they may have already a word for that.\nSubject-Verb-Object notation #  Allows having a consistent way to phrase activies our events in the domain.\nExample: Host checks current reservation.\n Subject: Host Verb: Checks Object: Reservation  Note that \u0026ldquo;current\u0026rdquo; can be seen as a modifier.\nSometimes there may be multiple objects. E.g., \u0026ldquo;Bartender collects Payment for a Drink Order\u0026rdquo;:\n Both are objects \u0026ldquo;Payment\u0026rdquo; is a direct object. \u0026ldquo;Drink Order\u0026rdquo; is a indirect object.  Regardless, diferentiation is not that relevant.\nBounded Context #  Ubiquitous Language and model for a sub-domain. The means that a concept might change from one bounded context to another.\nEach one has unique domain concepts and not all contexts are compatible with others.\nTypically Microservices are build around these contexts. Best way to determine is to:\n Define use-cases. Look how different groups of people interact with a given entity. Look for variations of the ubiquitous language as they may suggest a new context. Look for variations where informations starts to become or relevant or irrelevant.  Strongly separated bounded contexts result in smoother workflows.\nAnti-Corruption Layer #  Solves issues where coupling starts. This layer leaves right next to the bounded context to avoid leaking info from/to that bounded context.\nTypical example: Let\u0026rsquo;s put everything in the same bag.\nWhat happens is that we have a layer responsible for translating similar concepts from one bounded context to another.\nHow to implement: Abstract interface as it represents the contract in its purest way without compromising - Sometimes abstractions are too much indirection but do understand. :shrug:\n  This is also useful for legacy systems. In this case a Anti Corruption Layer would be preferable on either end.\nContext Map #  Are a way of of visualizating Bounded contexts and the relationships between them.\n   Arrows represent dependencies. Lines may be labelled to indicate the nature of the relationship.  TODO Discovery Process using Event Storming #  Types of Domain Activities #   Command: A request yet to happen and can be rejected. Usually delivered to a specific destination and changes the state of the domain. Events: Action that happened in the past. Hence they can not be rejected. Often broadcast to many destinations. Record a change to the state of the domain, often the result of a command (what are the alternatives?). Query: Requestfor information about the domain. Usually delievered to a specific destination. Do not change the state of the domain.  All of these represent the types of messages in a Reactive System and form the API for a Bounded Context.\nDomain Objects #   Value objects: Defined by its attribute. Immuatable. Messages in Reactive Systems are implemented as Value Objects. Entity: Defined by an unique identity. The fields may change but not its identitity. Are the source of truth - Actors in Akka or Entitities in Lagom  Monolith- Aggregate: Collection of domain objects bound to a root entity:\n Example: Person (Aggregate Root), Name (Aggregate), Address (Aggregate). Transactions should not span multiple aggregate roots. The Aggregate Root may change between bounded contexts. Aggregate Root == Root Entity. Good cadnidates for distribution in Reactive Systems. Question: How to determine?  Is the entity involved in more operations in the same bounded context? Does it make sense deleting other entities when this one is deleted? Will a single transaction span multiple entities?    Domain Abstractiosn #  Services #  Busines Logic encapsulated. Should be stateless otherwise they become an entity or a value object.\nShould be fairly thin.\nFactories #  Constructing domain object may not be trivial as they may have to access external resources (DBs, files, REst APIs, etc).\nRepositories #  Similar to factories but used to get or modify existing objects. They work often over databases but can work with files or Rest Apis (I actually prefer \u0026ldquo;Gateways\u0026rdquo; for Rest APIs).\nNote: Factories and Repositories can be the same in practice.\nHexagonal Architecutre #  Is not directly related with domain driven design but is very compatible.\nDomain is at the core and is at teh center becoming the architectural focus. Then there are ports to communicate with the domain exposed as API for the domain. INfrastructure contains adapters that map to the ports.\nLike an Onion\n Domain  API - The ports Infrastructure - Adapts incoming and outgoing traffic in to the ports.    Outer layers depends on inner layers. And inner layers have no knowledge of other layers.\n:thinking: This does not seem different from the typical layered design with DB -\u0026gt; Services -\u0026gt; API\nEnsures proper spearation of infrastructure from domain.\nThese layers may be modelled through packages or projects. Details are not important. The important thing is to make the domain portable.\nWould like a concrete example on how it really differents from the N tiered design.\n"});index.add({'id':10,'href':'/editors/emacs/','title':"Emacs",'section':"Editors",'content':"Org-Mode #  Org-protocol is cool and opens possibilities (like there weren\u0026rsquo;t enoguh :overwhelmed:):\n https://orgmode.org/worg/org-contrib/org-protocol.html#orgheadline8  Configuring #  Literate config is a thing but unfortunately as far as I researched it has to be contained in a single file. I rather have separate files per use-case.\nCool reference links:\n https://tecosaur.github.io/emacs-config/config.html http://doc.norang.ca/org-mode.html:w https://github.com/jethrokuan/dots/blob/0064ea2aab667f115a14ce48292731db46302c53/.doom.d/config.el#L495 https://github.com/nmartin84/.doom.d#orgb81fe7f https://github.com/howardabrams/dot-files/blob/master/emacs-org.org  "});index.add({'id':11,'href':'/system-design/event_sourcing/','title':"Event Sourcing",'section':"System Design",'content':"Event Sourcing (ES) #  In addition to persisting state, one persists audit logs. This captures the history. It is better to have this in a database as it provides transactionality.\nWary when persisting in the database and in-memory:\n Two potential source of truth if they disagree. Both must be updated in transactionality. A bug in the code may lead to both of them becoming out-of-sync.  Q: What happens if the audit logs gets out-of-sync with the state? A: The audit logs is the source of truth as, as opposition, is not possible to rebuild the audit logs from the current state.\nThis leads to not requiding storing the current state. Event Sourcing captures the intent.\nWhenever we need to obtain the current state we replay the logs until we reach the current state. Attention: We mustn\u0026rsquo;t also replay the side-effects.\nAppend-only is also more efficient in the databases - No deletions or updates.\nEvent Integrity: The logs are the most important thing and should never be rewritten -\u0026gt; immutable events.\nOptimization through snapshots #  What happens if the list of events is too large? Solution: Ocasionally persist a snapshot and we replay the events from that point on (issue: We may receive an event out-of-order, invalidating the snapshot).\nVersioning #  Issue when we have to change the event\u0026rsquo;s schema. This leads to ModelV1, ModelV2, etc. This requires supporting all versions. And requires flexibile formats: JSON, ProtoBuf or AkkA Event Adapters in the lightbend ecosystem that is between the system and the DB translating the V1, V2, VN to the corresponding and unique Domain entity.\nProblems #  When we need to perform queries that involve several aggregate roots. Because entities need to be rebuilt from events everytime they are visited.\n The model used to persist are not compatible with the model required during queries.\n See Command Query Responsibility Segregation.\nCommand Sourcing #  Similar to command sourcing but persists commands as opposed to events so:\n  Issue command\n  Persist command\n  Run asyncronous the command\n  They should be idempotent as they run multiple times (e.g., failures).\n  Must be validated so that they do not become stuck in the queue forever.\n  Bad: The sender might not be notified if the command fails due to the decouple nature.\n  "});index.add({'id':12,'href':'/protocols/gossip_protocol/','title':"Gossip Protocol",'section':"Protocols",'content':"Some of these text may be interwined with specifities of the Gossip Protocol within Akka.\nContext: TODO\nAt a regular interval, each member sends their view of the cluster state to a random node, including:\n The status of each member If each member has seen this version of the cluster state.  Eventually consistent as after some time (aka convergence), all nodes will share the same state. Each node decides if it has reached convergence.\nAfter it, each node will have the same ordered set of nodes:\n Node 1 Node 2 Node 3  Then the first eligible node, will become the leader and will perform leader\u0026rsquo;s duties such as notify member state changes\nNote: that each leader may change after each round if the cluster membership has changed.\nMember Lifecycle - Happy Path #  Joining -\u0026gt; Up -\u0026gt; Leaving -\u0026gt; Exiting -\u0026gt; Removed (tombstoned) In this case:\n Once all nodes know that all nodes know that a new node is \u0026ldquo;Joining\u0026rdquo;, it marks it as \u0026ldquo;Up\u0026rdquo;. A leaving node sets itself as Leaving, then once all nodes know it, the leader marks it as Exiting. Once all leaders know that a node is Exiting, the leader sets the node as Removed.  Member Lifecycle - Unhappy Path #  Joining -\u0026gt; Up -\u0026gt; Leaving -\u0026gt; Exiting -\u0026gt; Removed (tombstoned) | | | | /|\\ |--disconnected--------------| | \\|/ | Unreachable (F) ----\u0026gt; Down (F) ------------ When a new member is disconnected, it is marked as Unreachable until it recovers. If it is permanent, it is flagged as Down and it moves to state Removed and can never come back.\nEach node is monitored for failures by at most 5 nodes using Heartbeat. Once a member is deemed unreachable, then than information will be gossiped to the cluster. The member will remain like that until all nodes flag the node as Reachabe again.\nReasons:\n Crashes Heavy load Network Partition or failure  Impact of Unreachable Nodes #  Convergence will not be possible therefore no leaders will be elected therefore new nodes cannot fully join the cluster. This will happen until the node is marked as Down (and then Removed).\nIn this moment, potential new members will have a state WeaklyUpMember, which can transition to Up once convergence is complete. Note that, this member can be used by applications but should not if consistency is important (Cluster Shards or Cluster Singletons).\nTODO #   Check if Heartbeat and Gossip Protocol are always interwined.  "});index.add({'id':13,'href':'/concurrency/gunther_s_universal_scalability_law/','title':"Gunther's Universal Scalability Law",'section':"Concurrency",'content':"Increasing concurrency can cause negative resutrns due to contention and coherency delay.\nPicks from Amdah\u0026rsquo;s Law. In addition to contention, it accounts for coeherency delay.\nAs the system scales up, the cost to coordinate between nodes exceeds any benefits.\n"});index.add({'id':14,'href':'/learning/hands_on_scala_programming/','title':"Hands-on Scala Programming",'section':"Learning",'content':"Follows my notes on the Haoyi Li.\u0026rsquo;s book: \u0026ldquo;Hands-on Scala Programming\u0026rdquo; (https://www.handsonscala.com/).\nAs an experiment, I coding directly in the org-mode file using Babel to execute the Scala blocks:\nTODO Some introduction #   Point to dotfiles Point to the Babel package  Notes #   Lack of auto-complete when writing here. Compilation errors are hard to track. Workaround is to open a separate buffer with the ammonite REPL console. For sure will have to move to a dedicated project once I use Mill. Or maybe not.  Exercises 3.5 #  3.63 #   “Write a short program that prints each number from 1 to 100 on a new line.\nFor each multiple of 3, print \u0026ldquo;Fizz\u0026rdquo; instead of the number.\nFor each multiple of 5, print \u0026ldquo;Buzz\u0026rdquo; instead of the number.\nFor numbers which are multiples of both 3 and 5, print \u0026ldquo;FizzBuzz\u0026rdquo; instead of the number.”\n   \u0026ldquo;Define a def flexibleFizzBuzz method that takes a String =\u0026gt; Unit callback function as its argument, and allows the caller to decide what they want to do with the output. The caller can choose to ignore the output, println the output directly, or store the output in a previously-allocated array they already have handy.”\n def flexibleFizBuzz(callback: String =\u0026gt; Unit) { (1 to 100).foreach { i =\u0026gt; if (i % 3 == 0 \u0026amp;\u0026amp; i % 5 == 0) callback(\u0026#34;FizzBuzz\u0026#34;) else if (i % 3 == 0) callback(\u0026#34;Fizz\u0026#34;) else if (i % 5 == 0) callback(\u0026#34;Buzz\u0026#34;) else callback(i.toString) } } flexibleFizBuzz(s =\u0026gt; println(s)) 3.64 #   “Write a recursive method printMessages that can receive an array of Msg class instances, each with an optional parent ID, and use it to print out a threaded fashion. That means that child messages are print out indented underneath their parents, and the nesting can be arbitrarily deep.”\n  I assume that the items are ordered by their identifier which is incremental with each new message.   class Msg(val id: Int, val parent: Option[Int], val txt: String) def printMessages(messages: Array[Msg]) { val padding = 2 def printMessages(index: Int, msgLevel: Map[Int, Int]) { if (index \u0026lt; messages.length) { val message = messages(index) val level = message.parent match { case Some(parent) =\u0026gt; msgLevel.getOrElse(parent, 0) + 1 case None =\u0026gt; 0 } println(\u0026#34; \u0026#34; * level * padding + message.txt) printMessages(index + 1, msgLevel + (message.id -\u0026gt; level)) } } printMessages(index = 0, msgLevel = Map()) } printMessages(Array( new Msg(0, None, \u0026#34;Hello\u0026#34;), new Msg(1, Some(0), \u0026#34;World\u0026#34;), new Msg(2, None, \u0026#34;I am Cow\u0026#34;), new Msg(3, Some(2), \u0026#34;Hear me moo\u0026#34;), new Msg(4, Some(2), \u0026#34;Here I stand\u0026#34;), new Msg(5, Some(2), \u0026#34;I am Cow\u0026#34;), new Msg(6, Some(5), \u0026#34;Here me moo, moo\u0026#34;) )) "});index.add({'id':15,'href':'/protocols/heartbeat/','title':"Heartbeat",'section':"Protocols",'content':"In order to detect failures, systems communicate with one another to verify communication. If the communication is deemed broken, then the system may be considered as Unreachable depending on he heartbeat history and how the Failure Detection is configured. I.e., a single heartbeat does not mean that the member is Unreachable.\n"});index.add({'id':16,'href':'/security/jwt/','title':"JWT",'section':"Security",'content':"What it is #  JSON Web Tokens\nWhat it solves #  What it does not solve #  "});index.add({'id':17,'href':'/system-design/message_driven_architecture/','title':"Message Driven Architecture",'section':"System Design",'content':"Asyncronous and non-blocking. The sender does not actively wait for a response.\nAdvantages:\n Resources are freed immediatly. Reduced contention Messages can be queued for deleivery in case the receiver\u0026rsquo;s is offline. Provides a higher level of reliability.  Disavantages:\n Make transactions more difficult. How to manage long running transactions that span multiple microservices. Holding transactions open for long periods result in slow, brittle systems.  The role of syncronous messags:\n Can you acknowledge the message but process it asyncronously? The need for syncronous messages should be driven by domain requirements rather than technical convenience.  Sagas #  Represent long running transaction. Multiple requests are managed by a Saga.\nIndividual requests are run in sequence or in paralel.\nHow:\n Each request is paired with a compensating action. If any requests fails, compnesation actions are executed for all completed steps. Then the saga is completed with a failure. If compensation actions fails, then it is retried. This requires idempotency.  Timeouts in distributed systems:\n  Eitehr the request failed.\n  Either it was successful but the reply failed.\n  The request is still queued and may success or fail eventually.\nCompensating actions != Rollbacks.\nRollback: implies the transaction has not completed which removes the evidence of the transaction. Compensation: Applied on top of a previously completed action. Evidence of the orignal action remains.\n  Disavantages:\n Coupled to the failures unless we can move regardless of that. Saga are often implemented using ackka actors and represented via Finite State Machine.  Two General Problem #  Illustrate the impossibility of reaching a concensus over an unreliable communication channel.\nDelivery Guarantees #   Exactly Once: Is not possible in the event of a network partition, or lost message. We never guarantee that the message was in-fact sent. Failure requires resending the message which creates potential duplicates. Reuqires storage on both ends: unreliable network (as always); timeouts. At most once - If a failure occur, no retries are done which means no duplications but there may be losses. Requires no storage of messages. At least once - Require a acknoledge and teh sender needs to store state to track whether the messsage was acknowledge. It ahs to be stored in a durable data store.  Exactly once can be simulated using at least once and idempotency.\nAkka: at most once by default. Akka persistence has option to have at least once.\nMessaging Patterns #  Publish Subscribe #  Decoupled. The only coupling is on the message format and possibily the location (e.g., url, exchange on the message broker). Complexity is hard to see as we do not know where the message comes from.\nPoint to point #  Dependencies more clear but coupling is higher. COmplexity is directly observable.\nExamples #  Kafaka, RabbitMQ. Kafka allows point to point and pub/sub and we can even acknowledge once we finish processing.\nAkka: Typically point to point messaging;Persistence Query: Pub/sub Lagom: Point to point communication between services. Messages broker API allow for pub/sub.\n"});index.add({'id':18,'href':'/messaging-systems/messaging-systems-overview/','title':"Messaging Systems Comparison",'section':"Messaging Systems",'content':"Kafka #  Event Streaming, persistent.\nRabbitMQ #  Low latency.\nPulsar #  Made by Apache\nTODO #   Read this comparison: https://www.confluent.io/kafka-vs-pulsar/#:~:text=In%20reality%2C%20Kafka%2C%20RabbitMQ%2C,Pulsar%20sits%20somewhere%20in%20between.  "});index.add({'id':19,'href':'/system-design/microservices/','title':"Microservices",'section':"System Design",'content':"Subset of Service Oriented Architecture (SOA) where each service is deployed separately:\n Microservices can be physically separated and independently deployed. Each have its own data store. Independent and self governing. Communication is syncronous or asyncronous (e.g., through message brokers). Loose coupling between components (more or less by experience but that is design flaw likely :thinking:). Shorter development and release cycles. Each scale independently (either through physical or virtual machines).  Advantages #   Deployed/Scaled as needed. Increase availability due to reduced single point of failures. Better isolation leading to less couling giving flexibility. Supports multiple platforms and languages. More indepentent Shorter cycles  Mention that cross team coordination become less necessary. That is true but often it still is and requires coordination.\nDisavantages #   Require multiple complex deployment nad monitoring approaches. Cross service refactors are more challenging. Require supporting old versions. Organization Change  Designing one #  Business domains are typically large and complicated. Several ideas, actions and rules interact with one another in complex ways. A good approach on understanding such complex systems and how to model them is through a process named Domain Driven Design.\nA correct separation of concerns (i.e., clear and single responsibilities) leads to smoother workflows. Awkarwdness when modelling may indicate lack of understanding of the domain.\nPrinciples of Isolation #  State #  All access must go thorugh the API and there is no backdoors. Allows internal changes without affecting others.\nSpace #  Should not care where the other services are deployed. Allows microservices to be scaled up or down to meet demands. However, if latency is an issue, they should be in the same region.\nTime #  Should not wait for each other which allows more efficient use of resources. Resources can be freed immediatly, rather than waiting for a request to finish.\nBetween microservices we expect eventual consistency. It improves scalability as total consitency requires central coordination which hindes scalability.\nNOTE: This is discussible. However it may be a implementation details.\nFailure #  Isolate failures and should not cause failure on other systems. I.e., it still is responsive to attend other use-cases.\nIsolation Techniques #  Bulkheading #  Failures are isolated to failure zones. Failures in one service will not propagate to other services. Overall the system remains operation but possibly in a degraded state.\nIn practice it means that that systems that depend on the service that is considered a failure zone, will mark that information or service as unavailable. IMO this is tolerable if the service is non-critical.\nCircuit Breaker #  When a service is under stress we do not want to keep on retrying as it may make things worse.\nWay to avoid overloading a service. They qaurantine a failing service so it can fail fast. Allows the failing service to recover in its time before they fail.\nTypes:\n Closed - Normal Operation Open - Fail fast Half Open - Internally after a timeout will let one request to go through and if it fails, it goes back to Open.  Transitions:\n Closed (normal) -\u0026gt; Trip: Open (Fail Fast) Open (Fail fast) -\u0026gt; Attempt Reset: Half Open Half Open -\u0026gt; Trip: Open (fail Fast) Half Open -\u0026gt; Reset: Closed (Normal)  Message Driven Architecture #   Async and non blocking messages allows decoupling both time and failure. System do not depend on the response from on another. If a request to a service fails, the failure will not propagated. The client service isn\u0026rsquo;t waiting for response. It can continue to operate normally.  Autonomy #  Services can operate independently from each other.\nAutonomous services have enough information to resolve conflicts and repair failures. This means that they do not require other services to be operational all the time. Ideally all the time but in reality for a short time it guarantees some level of autonomy.\nBenefits:\n Stronger scalability and availability. Can be scaled indefinetly. Operate independently.  How:\n Async messages. Maintain enough internal state ofr the microservices to function in isolation. Use eventual consistency. Avoid direct, syncronous dependencies on external services.  API Gateway Services #  Microservices can lead to complexity in the API. How can we manage complex APIs that access many microservices?\nA new microservice that is put between the client and the N-services that are required to fulfill that request. This new microservice is responsible for aggregating the responses moving the responsbility from the client itself. This way, the client only needs to manage failures from the gateway.\nThis effectively creates an additional layer of isolation.\n!! This is specially useful for mobile applicatios where we cannot guarantee that the clients will update their app.\n"});index.add({'id':20,'href':'/system-design/monolith/','title':"Monolith",'section':"System Design",'content':"Charateristics #   Deployed as a single unit. No Clear Isolation. Complex Depedencies which in turn makes it hard to understand and modify. Big Bang Style Releases Long Cycle Times Careful releases Scalation is done with multiple copies and uses the database as consistency between them.  Advantages: #   Easy Cross Module Refactor Easier to maitain consistency Single Deploy Process Single thing to monitor Simple Scalability Model  Disavantages: #   Limited by the maximum size of a single phyisical machine. Only scales as the database allows. Components are scaled as a group. Deep coupling. Long Dev Cycle. Lack of reliaability given that one failure may impact the whole monolith.  Tearing it up #  Introduce domain boundaries within the application itself (e.g., libraries). This falls within Service Oriented Architecture (SOA).\n"});index.add({'id':21,'href':'/databases/overview/','title':"Overview",'section':"Databases",'content':"https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78\n"});index.add({'id':22,'href':'/work/random-memories/','title':"Random Memories",'section':"Work",'content':"Curious ideas/sentences and what not from now and then. Some things are pretty obvious but worthwhile remembering (:\n Failure in systems are inevitable. We have to accept and build a system that isolates such failures in a way that becomes unnoticible to the end-user. User does not care whether the software was faulty due to a third-party vender or not. When we build software we have to consider third-party vendors and if we don\u0026rsquo;t properly isolate ourselves from their faulty behavior, users are going to pay for that.     Responsiveness builds user confidence.\n  Domain: Sphere of knowledge that models either a business or an idea.\n  "});index.add({'id':23,'href':'/system-design/reactive_systems/','title':"Reactive Systems",'section':"System Design",'content':"Goal #  Provide an experience that is responsive under all conditions. Note that reactive programming is not the same as reactive systems.\nThis requires:\n Ability to scale from 10 users to million of users. Consume solely the resources required to support the current work-load.  Reactive Principles #  Systems that apply the following principles are considered reactive systems (see more here).\nResponsive #  Always respond in a timely manner.\nResiliency #  Isolate failures on single components - Similar to how a boat is designed.\nElastic #   Keep responsive specially when the system load changes which provides a more efficient usage of resources. Reduce contentions. Avoid central bottlenecks. Predictive Auto-Scaling policies.  Message Driven #   Losse coupling Isolatation - Kinda disagree with loose coupling, as the systems still will still depend on 3rd party behavior regardless of the communication medium. Resources are only consumed while active  Avoce all, make sure that the systems is not actively waiting so that it can do something else in the mean time which leads to a Async and non-blocking messages.\nGit as example #   Asyncronous and non-blocking as the work is submitted through PR and I can keep on working locally with no interruptions. Message based as it is basically \u0026ldquo;please review this\u0026rdquo;. Resiliency as each user has basically a copy of the whole repository locally. The local machine is isolated from the remote. Elastic has we can have multiple copies and does not cause problemas having that repository sync on that many machines. Responsive.  TODO https://www.lightbend.com/white-papers-and-reports/reactive-programming-versus-reactive-systems #  "});index.add({'id':24,'href':'/documentation/readme/','title':"Readme",'section':"Documentation",'content':"Several examples #   https://github.com/matiassingers/awesome-readme  Like minimalistic versions as they require less maintenance (hmmm :thinking-face:)\n"});index.add({'id':25,'href':'/system-design/service_oriented_architecture_soa/','title':"Service Oriented Architecture (SOA)",'section':"System Design",'content':"TODO very incomplete #  As opposed to Monolith, services do not share a database and all access must be done through a API exposed by the service. They may be in the same process (Monolith) or may be separated (Microservices). This reduces coupling.\n"});index.add({'id':26,'href':'/databases/databases_sharding-or-partitioning/','title':"Sharding or Partitioning",'section':"Databases",'content':"Technique used by some data stores to reduce contention without sacrificing consistency.\nRecords are distributed across nodes using a Shard Key or a Partition Key that will be used by the Database Router that redirects requests to the correct shard/partition.\nBenefits:\n Contention is isolated to a shard/partition. Given that each shard stores a part of the dataset, it is only handling a small part of the overall load. Improves elasticity.  However, with time the database will still be a bottleneck because there may be too many users acesssing the shards/partitions (sometimes it may be a bad choice of partition key).\n"});index.add({'id':27,'href':'/system-design/system-design_stateless/','title':"Stateless",'section':"System Design",'content':" Requests are self-contained and have all the information required to be completed. Requests can be processed on any instance of the application.  Some \u0026ldquo;Stateless\u0026rdquo; systems are not trully stateless as the state is contained in a database:\n Required to have strong consistency (the single source of truth). However this means that the database may become the bottleneck. The database also represents the single point of failure which may lead to an unresponsive systems.  "});index.add({'id':28,'href':'/data-processing/states-of-data/','title':"States of Data",'section':"Data Processing",'content':"States of data:\n At Rest: Data that is not consumed at the time is injested. It is stored and then consumed later in a batch process. In Transit: Data that is travelling between point A and point B. In Use: Data that is opened for treatment  Encountered several concerns regarding how such data must be handled. Follows a link to be reviewed later:three-states-of-data\n"});index.add({'id':29,'href':'/system-design/articles/','title':"System Design Articles",'section':"System Design",'content':" http://boringtechnology.club/  "});index.add({'id':30,'href':'/learning/technical_writing/','title':"Technical Writing",'section':"Learning",'content':"https://developers.google.com/tech-writing/overview\n"});index.add({'id':31,'href':'/work/thinking_tools/','title':"Thinking Tools",'section':"Work",'content':"Follows some interesting resources:\n https://untools.co/  "});index.add({'id':32,'href':'/work/way-of-work/','title':"Way of work",'section':"Work",'content':"It\u0026rsquo;s not about being right nor prove others wrong #   Corollary 1: if it\u0026rsquo;s wrong but it works, then it\u0026rsquo;s not wrong. Corollary 2: if you\u0026rsquo;re right but it doesn\u0026rsquo;t change the outcome, then it doesn\u0026rsquo;t matter. Corollary 3: if you\u0026rsquo;re right, but it doesn\u0026rsquo;t work, then you\u0026rsquo;re wrong. Corollary 4: if you prove someone else wrong, but their answer works and yours doesn\u0026rsquo;t, then they\u0026rsquo;re right and you\u0026rsquo;re wrong. Corollary 5: if you prove someone\u0026rsquo;s solution to be wrong even though it does provide value, then you have not yet provided any value until you propose something better.  Balancing Work/Life #  There is much in life beyond working. Follows an interesting article that contrasts with the tendency:\n https://1x.engineer/  "});index.add({'id':33,'href':'/snippets/emacs/','title':"Emacs",'section':"Snippets",'content':"TODO Personalize deft for other directories #  For now I do not need.\n;;function to run deft in specified directory (defun bjm-deft (dir) \u0026#34;Run deft in directory DIR\u0026#34; (setq deft-directory dir) (switch-to-buffer \u0026#34;*Deft*\u0026#34;) (kill-this-buffer) (deft) ) "});index.add({'id':34,'href':'/snippets/jackson/','title':"Jackson",'section':"Snippets",'content':"Sane Settings #  After working a while with this I want to register these sane defaults:\nconfigure(DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES, true) configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false) FAIL_ON_NULL_FOR_PRIMITIVES #  Setting FAIL_ON_NULL_FOR_PRIMITIVES forces clients to explicitely provide all values including primitives. Consider the following POJO:\ndata class Foo(bar: Boolean) Without the setting, a payload such as { } would render ~Foo(bar=false)~ despite the lack of default value. Given this, to guarantee consistency between the source-code and the external contract, I advise enabling this.\nFAIL_ON_UNKNOWN_PROPERTIES #  Setting FAIL_ON_UNKNOWN_PROPERTIES is useful when working on two systems in paralel. Settings this to false enables clients to send fields to the server that are not yet supported but will be. The alternative would be:\n Server includes those fields as optional fields (to avoid breaking current clients). Server roll-out. Clients update their HTTP clients to include the new fields. Once all-known clients support the new fields, make the same fields mandatory.  By setting this to true, this whole orchestration is not required.\nTODO Sub-Types #  Consider the following example that attempts to model a DSL that supports + and - operations.\nsealed class Expression data class Sum(val a: Int, val b: Int): Expression() data class Sub(val a: Int, val b: Int): Expression() data class Request( @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.EXTERNAL_PROPERTY, property = \u0026#34;type\u0026#34;, visible = true) @JsonSubTypes( JsonSubTypes.Type(value = Sum::class, name = \u0026#34;+\u0026#34;) JsonSubTypes.Type(value = Sub::class, name = \u0026#34;-\u0026#34;) ) val operation: Expression ) In short, this is saying if the JSON contains a field name type with value + it would deserialize to Sum and to Sub if type had value -. I.e., a sum operation between 1 and 2 would require the following JSON payload:\n{ \u0026#34;type\u0026#34;: \u0026#34;+\u0026#34;, \u0026#34;operation\u0026#34;: { \u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2 } } "});index.add({'id':35,'href':'/uncategorized/akka/','title':"Akka",'section':"Uncategorized",'content':"Akka #  Toolkit and runtime for building highly concurrent, distributed and fault tolerant message-driven application in the JVM. It can be used to build Reactive Systems.\nProposes unified programming model for:\n Simpler concurrency: single threaded ilusion as each actor processes a messsage at a time). Simpler distribution: is distributed by default). Simpler fualt tolerancy: Decouples communication from failure handling.  Akka Cluster #  Allows actors to communicate across the network, greatly simplifying the process. All members must share the same.\n What is the purpose of the name? :thinking:  Akka Cluster Aware Routers #  Context: High workload.\nScalling vertically has limits (including expenses). Introducing Akka Cluster Aware Routers, that allows work to be distributed across cluster. A large task is broken on smaller tasks that is routed to an instance of our application. This means that we may scale horizontally.\nAkka Cluster Sharding #  Context: Database becomes the bottleneck.\nMany applications leverage the database, specially to create consistency however this leads to contention (see Amdah\u0026rsquo;s Law). As the application scales, the database cannot keep up.\nAkka distributes actors across the cluster. And each actor maintains state for a specific database identifier. This eliminates database\u0026rsquo;s reads for a specific database identifier. The actor may reply directly. The actor model guarantees that the state and the database are always consistent (how exactly?).\n  Akka Distributed Data #  Context: Shared State/Data problem\nEither in the database, or sometimes offload to a dedicated cache service like Memcached or Redis, essentially moving the bottleneck. Additionally, it leads to additional infrastructure burden.\nAkka Distributed Data provides local, replicated, in-memory data storage.\n  The data is asyncronously replicated to another nodes, ensuring all nodes have access to the data. This is made with low latency and ensures fast updates across the cluster (Why given that each nodes is responsible for a specific database identifier? What happens if a node goes down?).\nOther #  CRDTs in distributed data are stored in memory. Can be copied to disk to speed up recovery if a replica fails.\nBest used for small data sets with infrequent updates that require high availability.\n A marker that shows something was deleted. Can result in data types that only get larger and never smaller. Aka CRDT Garbage  Limitations CRDT: Do not work with every data type that require a merge function. Some data types are too complex to merge and require the use of tombstone:\nAkka Address #    May be local or remote in the form: akka://\u0026lt;ActorSystem\u0026gt;@\u0026lt;HostName\u0026gt;:\u0026lt;Post\u0026gt;/\u0026lt;ActorPath\u0026gt;\nSeveral protocols are available and depend on the use-case:\n aeron-udp: High throughput and low latency (and probabilly lacks delivery guarantees?) tcp: Good thorughout and latency but lower. tls-tcp: When encryption is required.  Joining a Cluster #  Requires \u0026ldquo;Seed Nodes\u0026rdquo;, i.e., contact nodes. Any node is eligible. Best practice is to use \u0026ldquo;Akka Cluster Bootstrap\u0026rdquo; to avoid static configuration.\nMust be enabled! And it does not bring any advantage until we set the application to leverage this:\nval loyaltyActorSupervisor = ClusterSharding(system).start( \u0026#34;shared-region-name\u0026#34;, MyActorActor.props(someProp), ClusterShardingSettings(system), MyActorSupervisor.idExtractor, MyActorSupervisor.shardIdExtractor ) Akka Cluster Management #  Set of tools served through a HTTP Api to manage the cluster. Must start after the actor system.\nMust be enabled!\nAkka Discovery #  Service to locate and discover services.\nAkka Cluster Bootstrap #  Automated seed node discovery using Akka Discovery.\nHealth Check Endpoints #  Useful when integrating with orchestrating platforms (e.g., K8S).\nCommunication #  It is done by using Gossip Protocol.\nNetwork Partitions #  This issue cannot be recovered by simply rebooting the affected node. In order to fix this:\n Decide which partitions needs to be cleaned up - How? Shutdown the members Inform the cluster that those members are down - PUT -F operation=down /cluster/members/\u0026lt;member address\u0026gt;. Create new members to replace the old.  Step 2. is important otherwise it continues to operate unware that it has been removed from the cluster which can lead to multiple copies of the same shard.\nSplit Brain #    Occurs when single cluster splits into two or more distinctive clusters. It normally does not occur unless poor management (not stopping processes that are Down) or configuration (there are strategies to solve this automatically). Can be caused by improper Downing a member leading to the node creating another cluster as the process was not terminated.\nIt may also occur with a network partition. If this extend, the Unreachable Nodes will be marked as downed but will not be terminated.\nSimpler solutions may be solved automatically through orchestration platforms that automatically stop the process. More complicated split brains may be solved using Lightbend Split Brain Resolver.\nWhen using sharding or singleton for data consistency #  Each cluster can have a copy of the actor leading to a inconsistency and data corruption specially if both shards have access to the database.\nLighbend Split Brain Resolver #  Set of customizable strategies for terminating members in order to avoid Split Brain scenarios. Terminating members allow orchestration platforms to take over and heal the problem.\nStatic Quorum #    Fixed sized quorom of node. All nodes will evaluate their situation and Down unreachable. If quorum is set then a smaller cluster will prevail, otherwise the nodes will shutdown themselves. The quorum value must at least n/2 + 1.\nKeep Majority #  Similar to previous but dynamically tracks the size of the cluster.\nKeep Oldest #    Monitors the oldest node in the cluster. Members that are not communicating with that node will be marked as down and the nodes will terminate themselves. If the oldest node has crashed so will the cluster but is configurable in a way, that in that case only the oldest will be Downed.\nKeep Referee #  Similar to the other one but designate a specific node as referee (based on its address). As far as I can see, it is not configurable to avoid crashing the cluster if the referee is down.\nDown Allows #  All nodes terminate themselves relying on good orchestration tools to reduce downtime - Me not like this one.\nLease Majority #  Reserved for Kubernetes deployments.\n  It uses a distributed lock (lock) to make it\u0026rsquo;s decision. Each partition will attempt to obtain it the loser terminates and the winnner remains.\nThere is a bit of nice hack (IMO but can\u0026rsquo;t understand exactly how this is achieved) which is that the side that is theoretically smaller will delay the attempt to obtain the lock so that the majority wins.\nSome Edge Cases #     Indirect connected Edges (for some reason is connected to only one member). Unstable nodes (keeps on disconnecting from some nodes).  These edge-caes are automatically handled.\nOrphaned Node #  Is down but not terminated.\nTODO Cluster Singleton #  Akka Cluster Sharding #  Distribute actors across a cluster.\nEntities #    Unique within the cluster. Acts as a single source of truth leading to Strong Consistency.\nEntities agre grouped into Shards.\nTypically they correspond to the domain concept that the entity is modelling and the identifier is usually the aggregate root\u0026rsquo;s identifier (e.g. UserId).\nFor this purpose we can use Extractor that is often modeled using a Envelope (which is not mandatory if the mssage contains the identifier):\ncase class Envelope(entityId: String, message: Any) val idExtractor: ExtractEntityId = { case Envelope(id, msg) =\u0026gt; (id, msg) } Shard #    Holds entities. The distribution depends on a function which procuces the Shard Id (usually based on the Entity Id). Mapping entities to Shard Id dictates how to control the distribution of the entities across the cluster. Improper distribution may lead to hotspots in the cluster (unbalance).\nRule of thumb: ~10 shards per node. Too many shards have a cost to find them. And too short reduces capability to distribute the nodes on the cluster (e.g., 2 shard across 3 nodes).\nExample of unbalanced ids:\n Names of people: Creates hotspots for common names. Dates: Hotspots for recent dates. Auto-incrementing ids: Hotspots for recent ids.  Usually the best is Math.abs(hashCode % number_shards).\nShard Region #    Holds Shards. For a type of entity, there is usually one Shard Region per JVM.\nShard Coordinator #  Runs as a Akka Cluster Singleton. It is responsible to route the messages addressed to a specific entity. It provides the location of the shard which can then be used to acccess the entity.\nBacked by Akka Distributed Data or Akka Persistence.\nHandling Messages Asyncronously #  Blocking threads inside the actors creates contention therefore the handling of the messages must happen in a async fashion, including any DB writes and DB reads when starting the actor. For example, using interfaces such as Future[T]. However this may lead to concurrency within the actor itself removing the ilusion of a single thread. This means that messages must be stash until other operations complete.\nIn essence the actor has 3 states:\n Loading - Load the state from the DB. Running - Regular behavior. Waiting.    Question: If DB fails, then it is recommended to throw the exception leading to a restart of the Actor that in turn will re-read the state from the DB. So:\n It is explained that the stash is not lost, how? What happens if there is a persistent issue in the DB? Will there be a loop?  Passivation #  This fenomenon can be observer through small dips in the throughput. This happens as the Actors attempts to manage the number of actors in-memory as keep all of them is unreasonable. E.g., idle actors.\nEach actor tracks the time it processed a message. If it hadn\u0026rsquo;t processed a message within a configured time period, it will Passivate, leading to the removal of the actor in-memory.\nThe period must be tune-up, too long may lead to OOM and too short may lead to constant reads from the DB. Best practise is to determine and then tune up by watching the memory usage.\nIt can also be done manually by sending a Passivate message to the parent.\nRebalancing #  Occurs whenever the size of the cluster changes. The Shard coordinator will initiate the rebalancing process by distributing the shards across the all available nodes in order to keep an even distribution of entities.\nThis can only occur in a healthy cluster. Therefore any unreachable nodes must be removed (and terminated before) either manually through Akka Management or using the Lightbend Split Brain Resolver.\nSteps:\n Coordinator informs Regions that a rebalance has started. Messages to an entity on a moving shard are buffered. Oce shared was rebalanced, the queued messages are sent  Remember Entities #  The entities are not autoamtically restarted until they are requested again. It can be configured otherwise by enabling remember-entities. When a node restarts/rebalances, it will restore those entities. This works by informing every member every time each entity starts or stops (using Akka distributed data) and stored in a durable storage in the disk (it can be recovered even after full cluster restart) - Hmm, but what if this is running in K8S where the disks are ephemeral? :thinking:\nWarning!\n Enabling this disables automatic passivation. It is not cheap as every node will track every running entity and adds overhead when starting/stopping entities.  Best practice is to limit when we have a limited number of active entities. Most of times is not really needed as entities will be removed automatically through Passivation brought back when needed. However some use-cases:\n When the entity has a scheduled process that may not have completed. When the time to restart an entity on demand could cause the system to backup (long startup times). When the resource savings of passivating the Entities are insignificant.  With this feature, the node\u0026rsquo;s ExtractShardIf function must handle ShardRegion.StartEntity(entityId).\nSemantics #  Message delivery in Akka is is best effort (at-most-once, the default). Not sure if I like this :thinking:\nAkka Actor Lifecycle #  Can be stopped by himself or by others.\nLifecycle (without faults):\nhook:preStart() -\u0026gt; started -[stop]-\u0026gt; stopped -\u0026gt; hook:postStop() -\u0026gt; terminated The actor is created assyncronously and available right away through the ActorRef.\nStopping an actor will:\n Finishes the processing the current message. Suspends message processing. Stop its children - See Actor Model. Waits for their termination confirmations and then stops himself.  How to stop: PoisonPill (context.stop(self())) and actorRef ! Kill messages (throw ActorKilledException). They are not appropriate to perform a cleanup before shutting down as the Actor does not \u0026ldquo;see\u0026rdquo; those messages (it is handled internally). It is best to use a dedicated message such as StopMeGraciouslyMessage.\nMonitor #  Dead Watch allows monitoring another actor\u0026rsquo;s termination (regular Terminated message in the receive block).\nFailure Handling #  Akka deals with failures at the level of the individual actor (bulkheading has it only affects that actor).\nDoes not throw the message back to the sender (b/c the sender does not know how to handle it). Instead the error is sent to a responsibile entity (e.g., \u0026ldquo;Manager\u0026rdquo;) that determines the required steps to recover.\nWhen an actor fails, Akka provides two configurable strategies:\n OneForOneStrategy: Only the faulty child is affected. AllForOneStrategy: All children are affected by one faulty child.  Both are configured with a type Decider = PartialFunction[Throwable, Directive]. If not defined a directive, then the parent is consired faulty. Where Directive:\n Resume: Resume message processing. Use if the state remains valid. Restart: Start a new actor in its place and resume, however all childs are stopped (by default unless preRestart hook is changed). It supports max number of retries and within a time limit. Stop. Escalate: Delegate the decision to the supervisor\u0026rsquo;s parent.  By default it is OneForOneStrategy with some directives that are too specific to group here and we can check the documentation. In short, by default, the actor will be restarted. In any case, message processing is suspended.\n All descendants of the actor are suspended. The actor\u0026rsquo;s parent handles the failure.  Proper tuning leads to a self-healing system. Some exceptions are worth stopping the actor while others are worth recovering.\nFull Lifecycle #    Ask vs Tell #  Ask: actorRef ? Message Tell: actorRef ! Message\nUse Ask when:\n Bridging non-actor code to actor-code (e.g., bridging with HTTP controllers ?). We are expecting a response within a timeout. In this case we use actorRef ? Message pipeTo self which in turn will will handle the response, e.g., val receive: Receive = { case MessageResponse =\u0026gt; stuff }.  Use tell when:\n We do not care about the response.  TODO Router #  It is something I have more questions about :thinking:\nAkka Streams #  Use cases: live-data, ETL systems, streaming.\n It is more efficient to consume asyncronously. Avoid flooding a slow consumer (backpressure).  Initiative to provide a standard for async stream processing with non-blocking backpressure.\nComponents of a Reactive Stream:\n Publisher: Publishes data to stream Subscriber: Consumes data from the stream. Processor: Acts as both a publisher and a subscriber, obeying the contract for each. Subscription: Connects a subscriber to a publisher to initiate a message flow.  Streams of data are Messages that will be consumed by actors.\nHow #  Data flows through a chain of processing stages. Each stage has zero or more inputs/outputs depending on the type. By default these stages run sync inside a single actor but can also be configured to run asyncronously in separate actors.\nLinear Streams #     Sources: the source of the data in the stream. E.g., CSV. Sinks: the \u0026ldquo;destination\u0026rdquo; for the data. E.g., CSV file. Flows: Transformations. E.g., Concatenate columns. Runnable Graph: A stream where all the inputs and outputs are connected.  Each stage can be run sync or async. In most cases the order is preserved.\nBackpressure is propagated downstream stages to upstream.\n   Source\nStage with single output: Source[+Out, +Mat]:\n Out: The type of each element that is produced. Mat: Type of the materialized value. Usually NotUsed.  Source only push data as long as there is demand. The source will have to deal with incoming data until demand resumes (how how largely demands on the use-case).\nE.g., empty, single, repeat (repeat), tick (schedule), from iterables, cycle (same as iterable but repeats), stateful source ~unfold(initial)(fn), from actors, from files, tcp connections (!= data sent through it), java streams.\n     Sink\nStage with a single input: Sink[-In, +Mat]:\n In: The type of each element that is consumed. Mat The type of each element that is produced. E.g., Future[Int].  It creates backpressure by controlling Demand.\nE.g., ignore, foreach (pure side effects as it does not return values), head/last, headOption/lastOption, seq (materialized all elements), stateful sinks such as fold and reduce, actorRef (no backpressure mechanism), actorRefAck (provides backpressure), FileIO.toPath, StreamConverts.fromJavaStream.\nNote that if the stream is infinite, these sinks may never complete.\n     Flows\nSingle input and single output: Flow[-In, +Out, +Mat].\nActs both as producer and consumer therefore it propagates demand to the producer as well propagating (and transforming) messages produced to downstream stages.\nE.g., map, mapAsync(Parallelism) that still guarantees order, mayAsyncUnordered, mapConcat (similar to flatMap but not the same), grouped, sliding, fold, scan (emits each new computed result), filter, collect, limit by time using takeWithin(Duration), dropWithin(Duration), groupedWithin(Number, Duration), zip, flatMapConcat (similar mapConcat but operates on Sources rather than iterables), flatMapMerge (similar to flatMapContact but the substreams are consumed consumed therefore order is not guaranteed), buffer(size, strategy) (smooth incosistencies in the flow rate), for slow consumers/producers we have expand (extrapolate for slow producers), batch (for slow consumer) and conflate (create summary of the elements when the producer is faster), log.\nSome of these operations are directly accessible from Source and does not require additional typing.\n     Runnable Graphs\nConnects source, flows, sinks so that data can start flowing.\nE.g.,:\n to: materializes the value from left to right. - Returns Cancellable. toMat: Transform/combine materialized values: source.viaMat(flow)(Keep.right).to(sink).run() - Returns NotUsed. viaMat: Similar but operates on a flow as opposed to sink - Returns Future[Done].       Fault Tolerancy.\nDefault strategy is to stop processing the stream and can be overriden within the ActorMaterializer by passing a decider that given an exception it either decides:\n Stop: terminate with an error. Resume: Drop the failing element. Restart: The element is dropped and the stream continues after restarting the stage. Any state acumulated by that stage will be cleared.  Each stage can be fine-tuned to choose a dispatchet, bufferSize, log levels and supervision.\nSometimes errors are recoverable which is a Throwable -\u0026gt; T.\n     Graphs\nIntroduces Junctions which take multiple inputs and multiple outputs. Basic ones are:\n Fan in: N inputs + 1 output. E.g., Merge (random selects from the inputs), MergePreferred (give one input higher priority), ZipWith (arity N), Zip (arity 2), Concat. Fan out: 1 input + N outputs. E.g., Broadcast, Balance (to just one of the outputs), UnzipWithin to for example split a tuple to send to the outputs, UnZip splits a tuple[A, B] onto 2 streams A and B.         Fusion\nUntil now all of this runs syncronously as Akka \u0026ldquo;fuses\u0026rdquo; all stages onto a single syncronous one.\n  When fused on, buffers are not present in each stage. When turned-off each stage has a dedicated buffer once we start processing stages asyncronously.\nWe can disable fusing for everything however we can be more selective: Source(1 to 10).async.runForEach(println). This creates overhead of:\n Actors. Mailboxes. Buffers.  It is not a magic bullet. Performance gain/loss depends on the use-case.\nFusion optimization principles is: \u0026ldquo;insert an async boundary to bisect the stream into two subsections of roughly equal processing time.\u0026rdquo;. In other words, check at the current pipeline where the stages can be split so that they can be performed in paralell and joined almost at the same time. This implies looking at Grafana, analyse each stage and compare with the stream graph we have.\nLesson: Do not multitask on your computer while you are doing benchmarks :P This affected the exercises throughput by a lot.\n  Graphs #    Linear streams are most of the times sufficient. However, there are some cases where there are multiple inputs and outputs. Leading to additional components:\n Junctions: Branch points in the stream (e.g., fan-in, fan-out).  All the introduced components are immutable and can be reused in different scenarios as they solely contain instructions to do something with the data.\nExample #  Simple stream:\nSource(1 to 10) // source  .via(Flow[Int].map(_ * 2)) // stage  .to(Sink.foreach(println)) // sink  .run // materialize the graph Backpressure #      pull/push mechanism.\nSubscribers signal demand that is sent upstream via subscription. Publishers then push data (if available), this way publisher does not send more information than that it was demanded.\n  "});index.add({'id':36,'href':'/uncategorized/dotfiles/','title':"dotfiles",'section':"Uncategorized",'content':"Want to migrate to Nix.\nEmacs Configuration #  See more on Emacs (some reason it gives ambiguous reference).\n"});index.add({'id':37,'href':'/uncategorized/nix/','title':"Nix",'section':"Uncategorized",'content':" Reproducible Each reproduction (a build) produces a generation  Links:\n https://wiki.nikitavoloboev.xyz/package-managers/nix https://nixcloud.io/tour/?id=1 https://stephank.nl/p/2020-06-01-a-nix-primer-by-a-newcomer.html https://engineering.shopify.com/blogs/engineering/what-is-nix https://www.nmattia.com/posts/2018-03-21-nix-reproducible-setup-linux-macos.html https://shopify.engineering/what-is-nix  "});index.add({'id':38,'href':'/uncategorized/relevant-xkcds/','title':"Relevant xkcds",'section':"Uncategorized",'content':"On Standards #  https://xkcd.com/927/\n"});index.add({'id':39,'href':'/uncategorized/web-stack-enties/','title':"Stack Web Notes",'section':"Uncategorized",'content':"How To Drive Change as a Software Engineer #  Source: https://www.lihaoyi.com/post/HowToDriveChangeasaSoftwareEngineer.html\nThe Dark Side of Events - YouTube #  Source: https://www.youtube.com/watch?v=URYPpY3SgS8\u0026amp;feature=youtu.be\u0026amp;t=1884\nAsk HN: How to Take Good Notes? | Hacker News #  Source: https://news.ycombinator.com/item?id=22473209\nUnlearning toxic behaviors in a code review culture | by Sandya Sankarram | Medium #  Source: https://medium.com/@sandya.sankarram/unlearning-toxic-behaviors-in-a-code-review-culture-b7c295452a3c\nOrg Mode - Organize Your Life In Plain Text! #  Source: http://doc.norang.ca/org-mode.html#GettingOrgModeWithGit\n🧠 Own Your Second Brain: Set Up org-roam on Your Own Machine #  Source: https://www.ianjones.us/own-your-second-brain\nNicolas Mattia – Nix: A Reproducible Setup for Linux and macOS #  Source: https://www.nmattia.com/posts/2018-03-21-nix-reproducible-setup-linux-macos.html\nWriting a book: is it worth it? | Hacker News #  Source: https://news.ycombinator.com/item?id=24628549\nPresets | Starship #  Source: https://starship.rs/presets/#nerd-font-symbols\nMiniflux Hosting #  Source: https://miniflux.app/hosting.html\nRemoteRetro.org | Free. World-class. Agile retrospectives. #  Source: https://remoteretro.org/retros/\nWrite code that is easy to delete, not easy to\u0026hellip; — programming is terrible #  Source: https://programmingisterrible.com/post/139222674273/how-to-write-disposable-code-in-large-systems\nThe Product-Minded Software Engineer - The Pragmatic Engineer #  Source: https://blog.pragmaticengineer.com/the-product-minded-engineer/\nSSH Agent Explained #  Source: https://smallstep.com/blog/ssh-agent-explained/\nRemembering what you Read: Zettelkasten vs P.A.R.A. #  Source: https://www.zainrizvi.io/blog/remembering-what-you-read-zettelkasten-vs-para/\ndegoogle | A huge list of alternatives to Google products. Privacy tips, tricks, and links. #  Source: https://degoogle.jmoore.dev/#useful-links-tools-and-advice\nLPT_LISA - lisa19_maheshwari.pdf #  Source: https://www.usenix.org/sites/default/files/conference/protected-files/lisa19%5Fmaheshwari.pdf\n"});})();